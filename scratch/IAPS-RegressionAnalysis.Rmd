---
title: "IAPS Regression Analysis"
author: "Jonathan Trattner, Rachel Jones, and Delaney Teceno"
date: "7/26/2020"
output: 
  html_document:
    df_print: paged
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: true
      smooth_scroll: true
    theme: journal
    highlight: monochrome
    code_folding: hide
---

```{r setup, include=FALSE}
#Load packages
library(tidyverse)
library(fs)
library(tidymodels)
library(tictoc)
library(vip)
myGGTheme <- ggplot2::theme(plot.title = ggplot2::element_text(hjust=0.5, face = "bold.italic", size=16), #plot title aesthetics
                   plot.subtitle = ggplot2::element_text(hjust = 0.5, face = "bold", size = 12), #plot subtitle aesthetics
                   axis.title.x = ggplot2::element_text(size = 12, color= "black", face = "bold"), #x axis title aesthetics
                   axis.title.y = ggplot2::element_text(size = 12, color= "black", face = "bold"), #y axis title aesthetics
                   axis.text.x = ggplot2::element_text(angle = 0, hjust = 0.5, size = 12),
                   #legend aesthetics
                   legend.title = ggplot2::element_text(size= 14,
                                    color = "black",
                                    face = "bold"),
                   legend.title.align = 0.5,
                   legend.text = ggplot2::element_text(size = 10,
                                              color = "black",
                                              face = "bold"),
                   legend.text.align = 0)
```

## Read in Data

Below is the code to read in the data. The directories chunk is specific to the files' location on our computer and will need to be changed if run on another machine. The data is manipulated as it's read in -- please see the code comments for more detail.

```{r set directories}

dirIAPS <- '~/Desktop/IAPS/IAPS Data/' #set the directory for IAPS data
dirData <- '~/Desktop/IAPS/Subject Data/' #set the directory for subject data
dirImageOrder <- '~/Desktop/IAPS/Subject Image Order/' #set the directory for image order
filesIAPS <- dir_ls(dirIAPS) #list the files in the IAPS directory
filesData <- dir_ls(dirData) #list the files in the subject data directory
filesImageOrder <- dir_ls(dirImageOrder) #list the files in the image order directory
demographics <- read.csv(file = "C:/Users/rachjone/Documents/KLRIF_2018-07-09.csv")
d <- demographics %>%
  rename(subject = 'Subject.ID', age = 'Age..years.', gender = 'What.is.your.gender.')
d2 <- d %>%
  mutate(subject = str_replace(subject, "\\-", "\\_"))
d3 <- d2 %>%
  mutate(gender = recode(d2$gender, "Male" = 1, "Female" = 2))
d4 <- d3 %>%
  select (subject, age, gender)
```

```{r read data, message = FALSE, warning = FALSE}

#Import the IAPS key data set
keyIAPS <- filesIAPS %>%
  map_df(~read_csv(.x, na = c("", ".", NA),
                   col_types = cols(IAPS = col_character())) %>%
           select(desc:dom1sd) %>%
           drop_na(),
         .id = "form") %>%
  mutate(form = case_when(str_detect(form, "1_IAPS") ~ 1,
                          str_detect(form, "2_IAPS") ~ 2,
                          str_detect(form, "3_IAPS") ~ 3,
                          str_detect(form, "4_IAPS") ~ 4),
         form = as.factor(form))

#Read in subject rating
subjectData <- filesData %>%
  map_df(~read_csv(.x) %>%
           rename(time = Timestamp,
                  subject = `Subject ID:`),
         .id = "form") %>%
  mutate(form = case_when(str_detect(form, "1_SFA") ~ 1,
                          str_detect(form, "2_SFA") ~ 2,
                          str_detect(form, "3_SFA") ~ 3,
                          str_detect(form, "4_SFA") ~ 4),
           form = as.factor(form)) %>%
  #pivot the data so all ratings are in a column "rating" instead of in their own rows.
  pivot_longer(cols = -c(form, time, subject), values_to = "rating") %>%
  select(-name) %>%
  #have a running order for each subject
  group_by(subject) %>%
  mutate(order = row_number()) %>%
  ungroup() %>%
  #make this subjectID string match that of imageOrder
  mutate(subject = str_replace(subject, "\\-", "\\_")) %>%
  select(-time) %>%
  #arrange by each subject and the form (so one subject goes through all four
  #forms before moving to next subject)
  arrange(subject, form)


  imageOrder <- filesImageOrder %>%
  map_df(~read_csv(.x),
         .id = "form") %>%
  mutate(form = case_when(str_detect(form, "1_pictureID") ~ 1,
                          str_detect(form, "2_pictureID") ~ 2,
                          str_detect(form, "3_pictureID") ~ 3,
                          str_detect(form, "4_pictureID") ~ 4),
         form = as.factor(form)) %>%
  #pivot the data so all image values (IDs) are in a column "value"
  pivot_longer(cols = -c(X1, form),
               names_to = "subject") %>%
  #remvoe subject prefix
  mutate(subject = str_remove(.$subject, "\\d\\_")) %>%
  #arrange by subject
  arrange(subject) %>%
  #remove NAs that appear because of how the data was read in where each form's
  #image ID were basically in a new column for the subject with the prefix
  #1_SFA_..."
  drop_na() %>%
  #have a running order for each subject, and remove the X1 column.
  group_by(subject) %>%
  mutate(order = row_number()) %>%
  ungroup() %>%
  select(-X1)

```

## Tidy the Data

This section describes how we tidied the data in order to get it in a format to perform the elastic net regression. Please see code comments for more details.

```{r tidy data}

#join the imageOrder and subject data by subject, form, and order.
orderedSubjectData <- full_join(imageOrder, subjectData, by = c("form", "subject", "order")) %>%
  rename(picID = value)

#Label the orderedSubjectData with a column that has the type of question asked
#based on the picID's P or N. Then rename the picID column as IAPS, removing any
#"P" or "N" at the end, so the names are the same as in the keyIAPS dataframe.
labeledOrderedSubjectData <- orderedSubjectData %>%
  mutate(question = case_when(str_detect(picID, "P") ~ "positive",
                              str_detect(picID, "N") ~ "negative",
                              TRUE ~ "Oops"),
         picID = str_remove_all(picID, "P|N")) %>%
  rename(IAPS = picID)

#create positiveRatings as a dataframe that has IAPS data from the rounds where
#it was asked how positive the image made them feel. Then combine the rating and
#question data into one column that has the ratings for when the question was
#about positive feelings. I could've done this using the following code instead.
#Same thing.

#labeledOrderedSubjectData %>%
#  filter(question == "positive") %>%
#  select(-question) %>%
#  rename(positive = rating))

positiveRatings <- labeledOrderedSubjectData %>%
  filter(question == "positive") %>%
  pivot_wider(names_from = question, values_from = rating)

#create negativeRatings as a dataframe that has IAPS data from the rounds where
#it was asked how negative the image made them feel. Then combine the
#rating and question data into one column that has the ratings for when the
#question was about negative feelings.
negativeRatings <- labeledOrderedSubjectData %>%
  filter(question == "negative") %>%
  pivot_wider(names_from = question, values_from = rating)

#combine the positive and negative ratings data by IAPS, subject, and form.
#Could've just used IAPS but just more specificity. Then remove miscellaneous
#order column and join that with the IAPS key. Then remove NAs since there are
#some images participants saw that are not standardized in the IAPS key. The
#final data has 12 columns and 5760 rows and consists of 120 positive and
#negative ratings from 48 subjects.
finalData <- full_join(positiveRatings, negativeRatings, by = c("IAPS", "subject", "form")) %>%
  select(-c(order.x, order.y)) %>%
  full_join(keyIAPS, by = c("IAPS", "form")) %>%
  drop_na()

```

Below, we can see a preview of the data we will fit an elastic net regression on. This shows the first 10 rows for subject SFA_1025. The IAPS column is the picture ID. The positive and negative columns show the subject's ratings when asked "How positive does this image make you feel?" and "How negative does this image make you feel?". The desc column is a description of the IAPS image, and the remaining columns describe the mean and standard deviation for the image's standardized valence, arousal, and dominance, per the IAPS dataset.

```{r preview, echo = FALSE}

#preview the regression data frame for subject SFA_1025
head(finalData, 10)

```


## Modeling the Data

Forty-eight subjects rated 120 images from the International Affective Picture System database which provides normative ratings on valence, arousal, and dominance. Our goal is to predict these normative ratings based on subjects' responses to the questions "How positive does this image make you feel?" and "How negative does this image make you feel?" Below, we fit predictive models for this. This section is divided by predicted outcome (valence, arousal, and dominance), and each division has sections describing different models used. Specifically, we fit  elastic net and linear regression models. Model metrics and results are included for each predicted outcome. The code for each outcome is somewhat redundant. This is done to aid in reproducing the results for any predicted outcome.

### Valence

#### Elastic Net 

##### Creating a model specification and workflow

In order to set up the elastic net model, we split the data into 75% training and 25% testing. Please note that the data is not split up evenly. This is to say that, although there are a total of 48 subjects in the dataset who rated 120 different images, the training set contains a sample of ratings from each subject. Below is the code to describe how we perform 6 fold cross validation predicting the mean valence of the IAPS images given subjects' positive and negative ratings. We tune the model, testing every combination of penalty and mixture (lambda and alpha) as defined in the object `netGrid`.

```{r model set up}

#set a seed
set.seed(18)

#split the data into 75% training, 25% testing.
split <- initial_split(finalData)
#pull training data
train <- training(split)
#pull testing data
test <- testing(split)
#create 6 cross fold validation objects on the training data
CVtraining <- vfold_cv(train, v = 6)

#create a recipe object -- simply describing the formula to fit for predicting
#the mean valence from the positive and negative ratings.
netRec <- recipe(valmn ~ positive + negative, data = train)

#Create a grid to tune the model for penalty and mixture (lambda and alpha) in
#an elastic net regression.
netGrid <- expand_grid(penalty = c(seq(0, 1, by = 0.01), seq(2,10, by = 1)),
                       mixture = seq(0,1, by = 0.01))

#create a model specification -- a linear regression that tunes penalty and
#mixture. use the glmnet package
netSpec <- linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet")

#create a workflow object with the recipe and model specification
wf <- workflow() %>%
  add_recipe(netRec) %>%
  add_model(netSpec)

```

##### Tuning Model

```{r tune model, warning = FALSE, message = FALSE}

#tune original model
tic("tune grid")
netTuned <- tune_grid(wf,
                      resamples = CVtraining,
                      grid = netGrid)
toc()

```

This plot shows us that the lowest training RMSE for this elastic net regression is achieved when the penalty (lambda) is 0. This means a basic linear regression performs best. We fit this and observe the model metrics below.

```{r model metric plot, warning = FALSE, message = FALSE}
#preview penalty and mean RMSE/RSQ
netTuned %>% collect_metrics() %>%
ggplot(aes(penalty, mean, color = mixture)) +
  geom_point(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) + 
  labs(y = "Mean", x = "Penalty", color = "Mixture", title = "Average Metric and Penalty") +
  myGGTheme
```

This table shows the penalty and mixture and their relation to RMSE.
```{r collect model metrics}
#preview the lowest RMSE -- best metrics that yield it
netTuned %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  arrange(mean) %>% 
  head(10) %>% 
  select(-c(.estimator, n)) %>%
  rename(Penalty = penalty,
         Mixture = mixture,
         Metric = .metric,
         Mean = mean,
         `Std Error` = std_err)

```

##### Fit Best Model

As seen above, the parameters that best fit the data, as measured by the RMSE, are a lambda of `r netTuned %>% select_best(metric = "rmse") %>% pull(penalty)`, and an alpha of `r netTuned %>% select_best(metric = "rmse") %>% pull(mixture)`. We then use these parameters and predict on the testing data. Below are the performance metrics.

```{r fit best model}

#select best metrics from elastic net model
bestMetrics <- netTuned %>% select_best(metric = "rmse")

#finalize workflow with the best metrics
finalWF <- finalize_workflow(wf, parameters = bestMetrics)

#save last fit
testFit <- last_fit(finalWF, split = split)

#preview testFit metrics
print("Test Fit Metrics:")
testFit %>% collect_metrics() %>% 
  select(-.estimator) %>%
  rename(Metric = .metric, 
         Estimate = .estimate)

#Save model's predictions
Preds <- testFit %>%
  select(.predictions) %>%
  unnest(.predictions) %>%
  rename(Prediction = .pred,
         Actual = valmn)

#preview model's predictions
print("Preview of the predictions of best fitting model on mean valence per IAPS data:")
Preds %>% 
  select(Prediction, Actual) %>%
  head(50)


#Preview model metrics 
print("Model Beta Estimates for Elastic Net Regression:")
finalWF %>%
  fit(test) %>%
  pull_workflow_fit() %>%
  vi(lambda = bestMetrics$penalty) %>%
  select(-Sign)

print("Model Fit Metrics:")
metrics(Preds, truth = Actual, estimate = Prediction) %>% 
  select(-.estimator) %>%
  rename(Metric = .metric,
         Estimate = .estimate)

  
```  

#### Elastic Net - Interaction

##### Creating a model specification and workflow - Interact Model

We perform the same elastic net regression as described above, but this time we are trying to predict the mean valence given the positive and negative ratings, plus the interaction between the two. 

```{r model set up - interact}

#create a recipe object -- simply describing the formula to fit for predicting
#the mean valence from the positive and negative ratings. THEN add the
#interaction between positive and negative.
interactRec <- recipe(valmn ~ positive + negative, data = train) %>%
  step_interact(terms = ~ positive:negative)


#create a workflow object with the recipe and model specification
wfInteract <- workflow() %>%
  add_recipe(interactRec) %>%
  add_model(netSpec)

```

##### Tune Interact Model

```{r tune interact model, warning = FALSE, message = FALSE}

#tune the interact model
tic("tune interact grid")
netTunedInteract <- tune_grid(wfInteract,
                      resamples = CVtraining,
                      grid = netGrid)
toc()

```

Similar to the above plot, this shows us that the lowest training RMSE for this interact elastic net regression is achieved when the penalty (lambda) is 0. This means a basic linear regression performs best. We fit this and observe the model metrics below.

```{r interact metric plot, warning = FALSE, metric = FALSE}

#preview penalty and mean RMSE/RSQ
netTunedInteract %>% collect_metrics() %>%
ggplot(aes(penalty, mean, color = mixture)) +
  geom_point(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) + 
  labs(y = "Mean", x = "Penalty", color = "Mixture", title = "Average Metric and Penalty") +
  myGGTheme

```


```{r interact metrics}
#preview the lowest RMSE -- best metrics that yield it
print("Penalty and mixture and their relation to RMSE - Interact model:")
netTunedInteract %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  arrange(mean) %>% 
  select(-c(.estimator, n)) %>%
  rename(Penalty = penalty,
         Mixture = mixture,
         Metric = .metric,
         Mean = mean,
         `Std Error` = std_err) %>%
  head(10)
  

```

##### Fit Best Interact Model

As seen above, the parameters that best fit the data, as measured by the RMSE, are a lambda of `r netTunedInteract %>% select_best(metric = "rmse") %>% pull(penalty)`, and an alpha of `r netTunedInteract %>% select_best(metric = "rmse") %>% pull(mixture)`. We then use these parameters and predict on the testing data. Below are the performance metrics.

```{r fit best interact model}

#select best interact metrics
bestMetricsInteract <- netTunedInteract %>% select_best(metric = "rmse")

#finalize the workflow with the best interact metrics
finalWFInteract <- finalize_workflow(wf, parameters = bestMetricsInteract)

#save last fit
testFitInteract <- last_fit(finalWFInteract, split = split)

#preview testFit metrics
testFitInteract %>% 
  collect_metrics() %>% 
  select(-.estimator) %>%
  rename(Metric = .metric,
         Estimate = .estimate)

#Save interact model's predictions
InteractPreds <- testFitInteract %>%
  select(.predictions) %>%
  unnest(.predictions) %>%
  rename(interactPrediction = .pred,
         Actual = valmn)

#preview interact model's predictions
print("Preview of the predictions of best fitting interact model on mean valence per IAPS data:")
InteractPreds %>% 
  select(interactPrediction, Actual) %>%
  head(20)
  

#preview model metrics
print("Model Beta Estimates for Elastic Net Regression - Interact:")
finalWFInteract %>%
  fit(test) %>%
  pull_workflow_fit() %>%
  vi(lambda = bestMetrics$penalty) %>%
  select(-Sign) 
  
print("Interact Model Fit Metrics:")
metrics(InteractPreds, truth = Actual, estimate = interactPrediction) %>% 
  select(-.estimator) %>%
  rename(Metric = .metric,
         Estimate = .estimate)
  
```  

#### Linear Regression

Below, we are performing a basic linear regression predicting the mean valence rating on each IAPS image given subjects' positive and negative ratings. This time, no cross validation is used. We are fitting this model on the training dataset and evaluating the fit.

```{r linear reg, warning = FALSE, message = FALSE}

#Linear regression on training data, not cross validated
linReg <- lm(valmn ~ positive + negative, data = train)

#save linear reg predictions
LinRegPreds <- linReg %>% 
  predict(test) %>% 
  bind_cols(test) %>% 
  rename(linRegPrediction = `...1`,
         Actual = valmn)

#preview LinRegPreds
print("Preview of the predictions of the basic linear regression model on mean valence per IAPS data:")
LinRegPreds %>%
  select(linRegPrediction, Actual) %>%
  head(20)

#Preview Linear Regression Model Metrics
print("Model Fit Statistics for Basic Linear Regression:")
linReg %>% 
  tidy() %>% 
  rename(Term = term,
         Estimate = estimate,
         `Std Error` = std.error,
         Statistic = statistic,
         `P Value` = p.value)
print("Model Fit Statistics for Basic Linear Regression PT. 2:")
linReg %>%
  glance() %>%
  rename(`R Squared` = r.squared,
         `Adj. R Squared` = adj.r.squared,
         Sigma = sigma,
         Statistic = statistic,
         `P Value` = p.value,
         DF = df,
         Deviance = deviance,
         `DF Residual` = df.residual)
  
print("Model Fit Statistics for Basic Linear Regression PT. 3:")
metrics(data = LinRegPreds, truth = Actual, estimate = linRegPrediction) %>% 
  select(-.estimator) %>%
  rename(Metric = .metric,
         Estimate = .estimate)
  

```

It's interesting to note that the $R^2$ *are* slightly different in the above tables. I believe this is because the first one is passed to the `glance` function and describes the $R^2$ term relative to the training data, and the latter is relative to the testing data.

#### Interact Linear Regression

Below, we are performing a linear regression as described in the preceding section, but this time we are predicting the mean valence rating on each IAPS image given subjects' positive and negative ratings, in addition to their interaction.

```{r linear interact reg, warning = FALSE, message = FALSE}

#Linear regression on training data, not cross validated
linRegInteract <- lm(valmn ~ positive + negative + (positive * negative), data = train)

#save linear reg predictions
LinRegInteractPreds <- linRegInteract %>% 
  predict(test) %>% 
  bind_cols(test) %>% 
  rename(linRegInteractPrediction = `...1`,
         Actual = valmn)

#preview LinRegPreds
print("Preview of the predictions of the interact linear regression model on mean valence per IAPS data:")
LinRegInteractPreds %>%
  select(linRegInteractPrediction, Actual) %>%
  head(20)

#Preview Linear Regression Model Metrics
print("Model Fit Statistics for Basic Interact Linear Regression:")
linRegInteract %>% 
  tidy() %>% 
  rename(Term = term,
         Estimate = estimate,
         `Std Error` = std.error,
         Statistic = statistic,
         `P Value` = p.value)

print("Model Fit Statistics for Basic Interact Linear Regression PT. 2:")
linRegInteract %>%
  glance() %>%
  rename(`R Squared` = r.squared,
         `Adj. R Squared` = adj.r.squared,
         Sigma = sigma,
         Statistic = statistic,
         `P Value` = p.value,
         DF = df,
         Deviance = deviance,
         `DF Residual` = df.residual)

print("Model Fit Statistics for Basic Interact Linear Regression PT. 3:")
metrics(data = LinRegInteractPreds, truth = Actual, estimate = linRegInteractPrediction) %>% 
  select(-.estimator) %>%
  rename(Metric = .metric,
         Estimate = .estimate)

```

As with the regular linear regression, it's interesting to note that the $R^2$ *are* slightly different in the above tables. I believe this is because the first one is passed to the `glance` function and describes the $R^2$ term relative to the training data, and the latter is relative to the testing data.

#### Compare Model Metrics

Below, we are simply comparing the performance metrics (mean absolute error (MAE), root mean squared error (RMSE), and r-squared ($R^2$)) for each model.

```{r compare metrics}

#Bind the metrics from each model together, add a model identification column.
#Rename the output and make it look nice.

metricComparison <- bind_rows(metrics(data = Preds, truth = Actual, estimate = Prediction) %>% mutate(Model = "Elastic Net"),
          metrics(data = InteractPreds, truth = Actual, estimate = interactPrediction) %>% mutate(Model = "Interact, Elastic Net"),
          metrics(data = LinRegPreds, truth = Actual, estimate = linRegPrediction) %>% mutate(Model = "Linear Regression"),
          metrics(data = LinRegInteractPreds, truth = Actual, estimate = linRegInteractPrediction) %>% mutate(Model = "Interact Linear Regression")) %>% 
  rename(Metric = .metric,
         Estimate = .estimate) %>%
  select(-.estimator) %>%
  arrange(Metric, Estimate)

print("Comparison of Model Metrics:")
metricComparison
  
```

```{r pull RMSE metrics, include = FALSE}

#Filter the metricComparison for just the RMSE
RMSEMetrics <- 
  metricComparison %>%
  filter(Metric == "rmse")

```

#### Results

We can see from the comparison table above that the `r RMSEMetrics %>% slice(1) %>% pull(Model)` model performs best. It has a testing RMSE of `r RMSEMetrics %>% slice(1) %>% pull(Estimate) %>% round(3)`, relative to the, `r RMSEMetrics %>% slice(2) %>% pull(Model)`, which has an RMSE of `r RMSEMetrics %>% slice(2) %>% pull(Estimate) %>% round(3)`; `r RMSEMetrics %>% slice(3) %>% pull(Model)`, which has an RMSE of `r RMSEMetrics %>% slice(3) %>% pull(Estimate) %>% round(3)`; and `r RMSEMetrics %>% slice(4) %>% pull(Model)`, which has an RMSE of `r RMSEMetrics %>% slice(4) %>% pull(Estimate) %>% round(3)`.

```{r compare preds, include = FALSE}

## Compare Model Predictions
# Below, we can see the predictions of each model fit and compare them to the Actual mean valence.


#join the different models' predictions by valmn and reorder the columns
comparePreds <- full_join(Preds, InteractPreds, by = "Actual") %>% 
  full_join(LinRegPreds, by = "Actual") %>% 
  select(Actual, everything())

#preview comparePreds
print("Preview of the comparison in predictions of different model types:")
comparePreds %>% 
  head(10)


```


### Arousal

#### Elastic Net 

##### Creating a model specification and workflow

In order to set up the elastic net model, we split the data into 75% training and 25% testing. Please note that the data is not split up evenly. This is to say that, although there are a total of 48 subjects in the dataset who rated 120 different images, the training set contains a sample of ratings from each subject. Below is the code to describe how we perform 6 fold cross validation predicting the mean valence of the IAPS images given subjects' positive and negative ratings. We tune the model, testing every combination of penalty and mixture (lambda and alpha) as defined in the object `netGrid`.

```{r model set up - arousal}

#set a seed
set.seed(18)

#split the data into 75% training, 25% testing.
split <- initial_split(finalData)
#pull training data
train <- training(split)
#pull testing data
test <- testing(split)
#create 6 cross fold validation objects on the training data
CVtraining <- vfold_cv(train, v = 6)

#create a recipe object -- simply describing the formula to fit for predicting
#the mean valence from the positive and negative ratings.
netRec_arousal <- recipe(aromn ~ positive + negative, data = train)

#Create a grid to tune the model for penalty and mixture (lambda and alpha) in
#an elastic net regression.
netGrid <- expand_grid(penalty = c(seq(0, 1, by = 0.01), seq(2,10, by = 1)),
                       mixture = seq(0,1, by = 0.01))

#create a model specification -- a linear regression that tunes penalty and
#mixture. use the glmnet package
netSpec <- linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet")

#create a workflow object with the recipe and model specification
wf_arousal <- workflow() %>%
  add_recipe(netRec_arousal) %>%
  add_model(netSpec)

```

##### Tuning Model

```{r tune model - arousal, warning = FALSE, message = FALSE}

#tune original model
tic("tune grid")
netTuned_arousal <- tune_grid(wf_arousal,
                      resamples = CVtraining,
                      grid = netGrid)
toc()

```

This plot shows us that the lowest training RMSE for this elastic net regression is achieved when the penalty (lambda) is close to 0 and mixture is close to one. In the next section, we use the values for lambda and alpha to predict on the testing data.

```{r model metric plot - arousal, warning = FALSE, message = FALSE}
#preview penalty and mean RMSE/RSQ
netTuned_arousal %>% collect_metrics() %>%
ggplot(aes(penalty, mean, color = mixture)) +
  geom_point(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) + 
  labs(y = "Mean", x = "Penalty", color = "Mixture", title = "Average Metric and Penalty") +
  myGGTheme
```

```{r collect model metrics - arousal}
#preview the lowest RMSE -- best metrics that yield it
print("Penalty and mixture and their relation to RMSE:")
netTuned_arousal %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  arrange(mean) %>% 
  head(10) %>% 
  select(-c(.estimator, n)) %>%
  rename(Penalty = penalty,
         Mixture = mixture,
         Metric = .metric,
         Mean = mean,
         `Std Error` = std_err)

```

##### Fit Best Model

As seen above, the parameters that best fit the data, as measured by the RMSE, are a lambda of `r netTuned_arousal %>% select_best(metric = "rmse") %>% pull(penalty)`, and an alpha of `r netTuned_arousal %>% select_best(metric = "rmse") %>% pull(mixture)`. We then use these parameters and predict on the testing data. Below are the performance metrics.

```{r fit best model - arousal}

#select best metrics from elastic net model
bestMetrics_arousal <- netTuned_arousal %>% select_best(metric = "rmse")

#finalize workflow with the best metrics
finalWF_arousal <- finalize_workflow(wf_arousal, parameters = bestMetrics_arousal)

#save last fit
testFit_arousal <- last_fit(finalWF_arousal, split = split)

#preview testFit metrics
testFit_arousal %>% collect_metrics() %>% 
  select(-.estimator) %>%
  rename(Metric = .metric, 
         Estimate = .estimate)

#Save model's predictions
Preds_arousal <- testFit_arousal %>%
  select(.predictions) %>%
  unnest(.predictions) %>%
  rename(Prediction = .pred,
         Actual = aromn)

#preview model's predictions
  print("Preview of the predictions of best fitting model on mean arousal per IAPS data:")
Preds_arousal %>% 
  select(Prediction, Actual) %>%
  head(20)


#Preview model metrics 
print("Model Beta Estimates for Elastic Net Regression:")
finalWF_arousal %>%
  fit(test) %>%
  pull_workflow_fit() %>%
  vi(lambda = bestMetrics_arousal$penalty) %>%
  select(-Sign)

print("Model Fit Metrics:")
metrics(Preds_arousal, truth = Actual, estimate = Prediction) %>% 
  select(-.estimator) %>%
  rename(Metric = .metric,
         Estimate = .estimate)
  
```  

#### Elastic Net - Interaction

##### Creating a model specification and workflow - Interact Model

We perform the same elastic net regression as described above, but this time we are trying to predict the mean valence given the positive and negative ratings, plus the interaction between the two. 

```{r model set up - interact - arousal}

#create a recipe object -- simply describing the formula to fit for predicting
#the mean valence from the positive and negative ratings. THEN add the
#interaction between positive and negative.
interactRec_arousal <- recipe(aromn ~ positive + negative, data = train) %>%
  step_interact(terms = ~ positive:negative)


#create a workflow object with the recipe and model specification
wfInteract_arousal <- workflow() %>%
  add_recipe(interactRec_arousal) %>%
  add_model(netSpec)

```

##### Tune Interact Model

```{r tune interact model - arousal, warning = FALSE, message = FALSE}

#tune the interact model
tic("tune interact grid")
netTunedInteract_arousal <- tune_grid(wfInteract_arousal,
                      resamples = CVtraining,
                      grid = netGrid)
toc()

```

Similar to the above plot, this shows us that the lowest training RMSE for this interact elastic net regression is achieved when the penalty (lambda) is 0. This means a basic linear regression performs best. We fit this and observe the model metrics below.

```{r interact metric plot - arousal, warning = FALSE, metric = FALSE}

#preview penalty and mean RMSE/RSQ
netTunedInteract_arousal %>% collect_metrics() %>%
ggplot(aes(penalty, mean, color = mixture)) +
  geom_point(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) + 
  labs(y = "Mean", x = "Penalty", color = "Mixture", title = "Average Metric and Penalty") +
  myGGTheme

```


```{r interact metrics - arousal}
#preview the lowest RMSE -- best metrics that yield it
print("Penalty and mixture and their relation to RMSE - Interact model:")
netTunedInteract_arousal %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  arrange(mean) %>% 
  select(-c(.estimator, n)) %>%
  rename(Penalty = penalty,
         Mixture = mixture,
         Metric = .metric,
         Mean = mean,
         `Std Error` = std_err) %>%
  head(10)

```

##### Fit Best Interact Model

As seen above, the parameters that best fit the data, as measured by the RMSE, are a lambda of `r netTunedInteract_arousal %>% select_best(metric = "rmse") %>% pull(penalty)`, and an alpha of `r netTunedInteract_arousal %>% select_best(metric = "rmse") %>% pull(mixture)`. We then use these parameters and predict on the testing data. Below are the performance metrics.

```{r fit best interact model - arousal}

#select best interact metrics
bestMetricsInteract_arousal <- netTunedInteract_arousal %>% select_best(metric = "rmse")

#finalize the workflow with the best interact metrics
finalWFInteract_arousal <- finalize_workflow(wfInteract_arousal, parameters = bestMetricsInteract_arousal)

#save last fit
testFitInteract_arousal <- last_fit(finalWFInteract_arousal, split = split)

#preview testFit metrics
testFitInteract_arousal %>% 
  collect_metrics() %>% 
  select(-.estimator) %>%
  rename(Metric = .metric,
         Estimate = .estimate)

#Save interact model's predictions
InteractPreds_arousal <- testFitInteract_arousal %>%
  select(.predictions) %>%
  unnest(.predictions) %>%
  rename(interactPrediction = .pred,
         Actual = aromn)

#preview interact model's predictions
print("Preview of the predictions of best fitting interact model on mean arousal per IAPS data:")
InteractPreds_arousal %>% 
  select(interactPrediction, Actual) %>%
  head(20)

#preview model metrics
print("Model Beta Estimates for Elastic Net Regression - Interact:")
finalWFInteract_arousal %>%
  fit(test) %>%
  pull_workflow_fit() %>%
  vi(lambda = bestMetrics_arousal$penalty) %>%
  select(-Sign)

print("Interact Model Fit Metrics:")
metrics(InteractPreds_arousal, truth = Actual, estimate = interactPrediction) %>% 
  select(-.estimator) %>%
  rename(Metric = .metric,
         Estimate = .estimate)
  
```  

#### Linear Regression

Below, we are performing a basic linear regression predicting the mean arousal rating on each IAPS image given subjects' positive and negative ratings. This time, no cross validation is used. We are fitting this model on the training dataset and evaluating the fit.

```{r linear reg - arousal, warning = FALSE, message = FALSE}

#Linear regression on training data, not cross validated
linReg_arousal <- lm(aromn ~ positive + negative, data = train)

#save linear reg predictions
LinRegPreds_arousal <- linReg_arousal %>% 
  predict(test) %>% 
  bind_cols(test) %>% 
  rename(linRegPrediction = `...1`,
         Actual = aromn)

#preview LinRegPreds
print("Preview of the predictions of the basic linear regression model on mean arousal per IAPS data:")
LinRegPreds_arousal %>%
  select(linRegPrediction, Actual) %>%
  head(20)

#Preview Linear Regression Model Metrics
print("Model Fit Statistics for Basic Linear Regression:")
linReg_arousal %>% 
  tidy() %>% 
  rename(Term = term,
         Estimate = estimate,
         `Std Error` = std.error,
         Statistic = statistic,
         `P Value` = p.value)

print("Model Fit Statistics for Basic Linear Regression PT. 2:")
linReg_arousal %>%
  glance() %>%
  rename(`R Squared` = r.squared,
         `Adj. R Squared` = adj.r.squared,
         Sigma = sigma,
         Statistic = statistic,
         `P Value` = p.value,
         DF = df,
         Deviance = deviance,
         `DF Residual` = df.residual)

print("Model Fit Statistics for Basic Linear Regression PT. 3:")
metrics(data = LinRegPreds_arousal, truth = Actual, estimate = linRegPrediction) %>% 
  select(-.estimator) %>%
  rename(Metric = .metric,
         Estimate = .estimate)

```

It's interesting to note that the $R^2$ *are* slightly different in the above tables. I believe this is because the first one is passed to the `glance` function and describes the $R^2$ term relative to the training data, and the latter is relative to the testing data.

#### Interact Linear Regression

Below, we are performing a linear regression as described in the preceding section, but this time we are predicting the mean valence rating on each IAPS image given subjects' positive and negative ratings, in addition to their interaction.

```{r linear interact reg - arousal, warning = FALSE, message = FALSE}

#Linear regression on training data, not cross validated
linRegInteract_arousal <- lm(aromn ~ positive + negative + (positive * negative), data = train)

#save linear reg predictions
LinRegInteractPreds_arousal <- linRegInteract_arousal %>% 
  predict(test) %>% 
  bind_cols(test) %>% 
  rename(linRegInteractPrediction = `...1`,
         Actual = aromn)

#preview LinRegPreds
print("Preview of the predictions of the interact linear regression model on mean arousal per IAPS data:")
LinRegInteractPreds_arousal %>%
  select(linRegInteractPrediction, Actual) %>%
  head(20)

#Preview Linear Regression Model Metrics
print("Model Fit Statistics for Basic Interact Linear Regression:")
linRegInteract_arousal %>% 
  tidy() %>% 
  rename(Term = term,
         Estimate = estimate,
         `Std Error` = std.error,
         Statistic = statistic,
         `P Value` = p.value)

print("Model Fit Statistics for Basic Interact Linear Regression PT. 2:")
linRegInteract_arousal %>%
  glance() %>%
  rename(`R Squared` = r.squared,
         `Adj. R Squared` = adj.r.squared,
         Sigma = sigma,
         Statistic = statistic,
         `P Value` = p.value,
         DF = df,
         Deviance = deviance,
         `DF Residual` = df.residual)

print("Model Fit Statistics for Basic Interact Linear Regression PT. 3:")
metrics(data = LinRegInteractPreds_arousal, truth = Actual, estimate = linRegInteractPrediction) %>% 
  select(-.estimator) %>%
  rename(Metric = .metric,
         Estimate = .estimate)

```

As with the regular linear regression, it's interesting to note that the $R^2$ *are* slightly different in the above tables. I believe this is because the first one is passed to the `glance` function and describes the $R^2$ term relative to the training data, and the latter is relative to the testing data.

#### Compare Model Metrics

Below, we are simply comparing the performance metrics (mean absolute error (MAE), root mean squared error (RMSE), and r-squared ($R^2$)) for each model.

```{r compare metrics - arousal}

#Bind the metrics from each model together, add a model identification column.
#Rename the output and make it look nice.

metricComparison_arousal <- bind_rows(metrics(data = Preds_arousal, 
                                      truth = Actual, 
                                      estimate = Prediction) %>% mutate(Model = "Elastic Net"),
                              metrics(data = InteractPreds_arousal, 
                                      truth = Actual, 
                                      estimate = interactPrediction) %>% mutate(Model = "Interact, Elastic Net"),
                              metrics(data = LinRegPreds_arousal, 
                                      truth = Actual, 
                                      estimate = linRegPrediction) %>% mutate(Model = "Linear Regression"),
                              metrics(data = LinRegInteractPreds_arousal, 
                                      truth = Actual, 
                                      estimate = linRegInteractPrediction) %>% mutate(Model = "Interact Linear Regression")) %>% 
  rename(Metric = .metric,
         Estimate = .estimate) %>%
  select(-.estimator) %>%
  arrange(Metric, Estimate)

print("Comparison of Model Metrics:")
metricComparison_arousal
  
```

```{r pull RMSE metrics - arousal, include = FALSE}

#Filter the metricComparison for just the RMSE
RMSEMetrics_arousal <- 
  metricComparison_arousal %>%
  filter(Metric == "rmse")

```

#### Results

We can see from the comparison table above that the `r RMSEMetrics_arousal %>% slice(1) %>% pull(Model)` model performs best. It has a testing RMSE of `r RMSEMetrics_arousal %>% slice(1) %>% pull(Estimate) %>% round(3)`, relative to the, `r RMSEMetrics_arousal %>% slice(2) %>% pull(Model)`, which has an RMSE of `r RMSEMetrics_arousal %>% slice(2) %>% pull(Estimate) %>% round(3)`; `r RMSEMetrics_arousal %>% slice(3) %>% pull(Model)`, which has an RMSE of `r RMSEMetrics_arousal %>% slice(3) %>% pull(Estimate) %>% round(3)`; and `r RMSEMetrics_arousal %>% slice(4) %>% pull(Model)`, which has an RMSE of `r RMSEMetrics_arousal %>% slice(4) %>% pull(Estimate) %>% round(3)`.

```{r compare preds - arousal, include = FALSE}

## Compare Model Predictions
# Below, we can see the predictions of each model fit and compare them to the Actual mean valence.


#join the different models' predictions by aromn and reorder the columns
comparePreds_arousal <- full_join(Preds_arousal, InteractPreds_arousal, by = "Actual") %>% 
  full_join(LinRegPreds_arousal, by = "Actual") %>% 
  select(Actual, everything())

#preview comparePreds
print("Preview of the comparison in predictions of different model types:")
comparePreds_arousal %>% 
  head(10)


```



### Dominance

#### Elastic Net 

##### Creating a model specification and workflow

In order to set up the elastic net model, we split the data into 75% training and 25% testing. Please note that the data is not split up evenly. This is to say that, although there are a total of 48 subjects in the dataset who rated 120 different images, the training set contains a sample of ratings from each subject. Below is the code to describe how we perform 6 fold cross validation predicting the mean valence of the IAPS images given subjects' positive and negative ratings. We tune the model, testing every combination of penalty and mixture (lambda and alpha) as defined in the object `netGrid`.

```{r model set up - dominance}

#set a seed
set.seed(18)

#split the data into 75% training, 25% testing.
split <- initial_split(finalData)
#pull training data
train <- training(split)
#pull testing data
test <- testing(split)
#create 6 cross fold validation objects on the training data
CVtraining <- vfold_cv(train, v = 6)

#create a recipe object -- simply describing the formula to fit for predicting
#the mean valence from the positive and negative ratings.
netRec_dominance <- recipe(dom1mn ~ positive + negative, data = train)

#Create a grid to tune the model for penalty and mixture (lambda and alpha) in
#an elastic net regression.
netGrid <- expand_grid(penalty = c(seq(0, 1, by = 0.01), seq(2,10, by = 1)),
                       mixture = seq(0,1, by = 0.01))

#create a model specification -- a linear regression that tunes penalty and
#mixture. use the glmnet package
netSpec <- linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet")

#create a workflow object with the recipe and model specification
wf_dominance <- workflow() %>%
  add_recipe(netRec_dominance) %>%
  add_model(netSpec)

```

##### Tuning Model

```{r tune model - dominance, warning = FALSE, message = FALSE}

#tune original model
tic("tune grid")
netTuned_dominance <- tune_grid(wf,
                      resamples = CVtraining,
                      grid = netGrid)
toc()

```

This plot shows us that the lowest training RMSE for this elastic net regression is achieved when the penalty (lambda) is 0. This means a basic linear regression performs best. We fit this and observe the model metrics below.

```{r model metric plot - dominance, warning = FALSE, message = FALSE}
#preview penalty and mean RMSE/RSQ
netTuned_dominance %>% collect_metrics() %>%
ggplot(aes(penalty, mean, color = mixture)) +
  geom_point(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) + 
  labs(y = "Mean", x = "Penalty", color = "Mixture", title = "Average Metric and Penalty") +
  myGGTheme

```

```{r collect model metrics - dominance}
#preview the lowest RMSE -- best metrics that yield it
print("Penalty and mixture and their relation to RMSE:")
netTuned_dominance %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  arrange(mean) %>% 
  head(10) %>% 
  select(-c(.estimator, n)) %>%
  rename(Penalty = penalty,
         Mixture = mixture,
         Metric = .metric,
         Mean = mean,
         `Std Error` = std_err)

```

##### Fit Best Model

As seen above, the parameters that best fit the data, as measured by the RMSE, are a lambda of `r netTuned_dominance %>% select_best(metric = "rmse") %>% pull(penalty)`, and an alpha of `r netTuned_dominance %>% select_best(metric = "rmse") %>% pull(mixture)`. We then use these parameters and predict on the testing data. Below are the performance metrics.

```{r fit best model - dominance}

#select best metrics from elastic net model
bestMetrics_dominance <- netTuned_dominance %>% select_best(metric = "rmse")

#finalize workflow with the best metrics
finalWF_dominance <- finalize_workflow(wf_dominance, parameters = bestMetrics_dominance)

#save last fit
testFit_dominance <- last_fit(finalWF_dominance, split = split)

#preview testFit metrics
testFit_dominance %>% collect_metrics() %>% 
  select(-.estimator) %>%
  rename(Metric = .metric, 
         Estimate = .estimate)

#Save model's predictions
Preds_dominance <- testFit_dominance %>%
  select(.predictions) %>%
  unnest(.predictions) %>%
  rename(Prediction = .pred,
         Actual = dom1mn)

#preview model's predictions
print("Preview of the predictions of best fitting model on mean dominance per IAPS data:")
Preds_dominance %>% 
  select(Prediction, Actual) %>%
  head(20)

#Preview model metrics 
print("Model Beta Estimates for Elastic Net Regression:")
finalWF_dominance %>%
  fit(test) %>%
  pull_workflow_fit() %>%
  vi(lambda = bestMetrics_dominance$penalty) %>%
  select(-Sign)


  print("Model Fit Metrics:")
metrics(Preds_dominance, truth = Actual, estimate = Prediction) %>% 
  select(-.estimator) %>%
  rename(Metric = .metric,
         Estimate = .estimate)
  
```  

#### Elastic Net - Interaction

##### Creating a model specification and workflow - Interact Model

We perform the same elastic net regression as described above, but this time we are trying to predict the mean valence given the positive and negative ratings, plus the interaction between the two. 

```{r model set up - interact - dominance}

#create a recipe object -- simply describing the formula to fit for predicting
#the mean valence from the positive and negative ratings. THEN add the
#interaction between positive and negative.
interactRec_dominance <- recipe(dom1mn ~ positive + negative, data = train) %>%
  step_interact(terms = ~ positive:negative)


#create a workflow object with the recipe and model specification
wfInteract_dominance <- workflow() %>%
  add_recipe(interactRec_dominance) %>%
  add_model(netSpec)

```

##### Tune Interact Model

```{r tune interact model - dominance, warning = FALSE, message = FALSE}

#tune the interact model
tic("tune interact grid")
netTunedInteract_dominance <- tune_grid(wfInteract_dominance,
                      resamples = CVtraining,
                      grid = netGrid)
toc()

```

Similar to the above plot, this shows us that the lowest training RMSE for this interact elastic net regression is achieved when the penalty (lambda) is 0. This means a basic linear regression performs best. We fit this and observe the model metrics below.

```{r interact metric plot - dominance, warning = FALSE, metric = FALSE}

#preview penalty and mean RMSE/RSQ
netTunedInteract_dominance %>% collect_metrics() %>%
ggplot(aes(penalty, mean, color = mixture)) +
  geom_point(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) + 
  labs(y = "Mean", x = "Penalty", color = "Mixture", title = "Average Metric and Penalty") +
  myGGTheme

```


```{r interact metrics - dominance}
#preview the lowest RMSE -- best metrics that yield it
print("Penalty and mixture and their relation to RMSE - Interact model:")
netTunedInteract_dominance %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  arrange(mean) %>% 
  select(-c(.estimator, n)) %>%
  rename(Penalty = penalty,
         Mixture = mixture,
         Metric = .metric,
         Mean = mean,
         `Std Error` = std_err) %>%
  head(10)

```

##### Fit Best Interact Model

As seen above, the parameters that best fit the data, as measured by the RMSE, are a lambda of `r netTunedInteract_dominance %>% select_best(metric = "rmse") %>% pull(penalty)`, and an alpha of `r netTunedInteract_dominance %>% select_best(metric = "rmse") %>% pull(mixture)`. We then use these parameters and predict on the testing data. Below are the performance metrics.

```{r fit best interact model - dominance}

#select best interact metrics
bestMetricsInteract_dominance <- netTunedInteract_dominance %>% select_best(metric = "rmse")

#finalize the workflow with the best interact metrics
finalWFInteract_dominance <- finalize_workflow(wfInteract_dominance, parameters = bestMetricsInteract_dominance)

#save last fit
testFitInteract_dominance <- last_fit(finalWFInteract_dominance, split = split)

#preview testFit metrics
testFitInteract_dominance %>% 
  collect_metrics() %>% 
  select(-.estimator) %>%
  rename(Metric = .metric,
         Estimate = .estimate)

#Save interact model's predictions
InteractPreds_dominance <- testFitInteract_dominance %>%
  select(.predictions) %>%
  unnest(.predictions) %>%
  rename(interactPrediction = .pred,
         Actual = dom1mn)

#preview interact model's predictions
print("Preview of the predictions of best fitting interact model on mean dominance per IAPS data:")
InteractPreds_dominance %>% 
  select(interactPrediction, Actual) %>%
  head(20) %>%

#preview model metrics
print("Model Beta Estimates for Elastic Net Regression - Interact:")
finalWFInteract_dominance %>%
  fit(test) %>%
  pull_workflow_fit() %>%
  vi(lambda = bestMetrics_dominance$penalty) %>%
  select(-Sign)

print("Interact Model Fit Metrics:")
metrics(InteractPreds_dominance, truth = Actual, estimate = interactPrediction) %>% 
  select(-.estimator) %>%
  rename(Metric = .metric,
         Estimate = .estimate)
  
```  


#### Linear Regression

Below, we are performing a basic linear regression predicting the mean arousal rating on each IAPS image given subjects' positive and negative ratings. This time, no cross validation is used. We are fitting this model on the training dataset and evaluating the fit.

```{r linear reg - dominance, warning = FALSE, message = FALSE}

#Linear regression on training data, not cross validated
linReg_dominance <- lm(dom1mn ~ positive + negative, data = train)

#save linear reg predictions
LinRegPreds_dominance <- linReg_dominance %>% 
  predict(test) %>% 
  bind_cols(test) %>% 
  rename(linRegPrediction = `...1`,
         Actual = dom1mn)

#preview LinRegPreds
print("Preview of the predictions of the basic linear regression model on mean arousal per IAPS data:")
LinRegPreds_dominance %>%
  select(linRegPrediction, Actual) %>%
  head(20)

#Preview Linear Regression Model Metrics
print("Model Fit Statistics for Basic Linear Regression:")
linReg_dominance %>% 
  tidy() %>% 
  rename(Term = term,
         Estimate = estimate,
         `Std Error` = std.error,
         Statistic = statistic,
         `P Value` = p.value) %>%

print("Model Fit Statistics for Basic Linear Regression PT. 2:")
linReg_dominance %>%
  glance() %>%
  rename(`R Squared` = r.squared,
         `Adj. R Squared` = adj.r.squared,
         Sigma = sigma,
         Statistic = statistic,
         `P Value` = p.value,
         DF = df,
         Deviance = deviance,
         `DF Residual` = df.residual)

print("Model Fit Statistics for Basic Linear Regression PT. 3:")
metrics(data = LinRegPreds_dominance, truth = Actual, estimate = linRegPrediction) %>% 
  select(-.estimator) %>%
  rename(Metric = .metric,
         Estimate = .estimate)

```

It's interesting to note that the $R^2$ *are* slightly different in the above tables. I believe this is because the first one is passed to the `glance` function and describes the $R^2$ term relative to the training data, and the latter is relative to the testing data.

#### Interact Linear Regression

Below, we are performing a linear regression as described in the preceding section, but this time we are predicting the mean valence rating on each IAPS image given subjects' positive and negative ratings, in addition to their interaction.

```{r linear interact reg - dominance, warning = FALSE, message = FALSE}

#Linear regression on training data, not cross validated
linRegInteract_dominance <- lm(dom1mn ~ positive + negative + (positive * negative), data = train)

#save linear reg predictions
LinRegInteractPreds_dominance <- linRegInteract_dominance %>% 
  predict(test) %>% 
  bind_cols(test) %>%
  rename(linRegInteractPrediction = `...1`,
         Actual = dom1mn)

#preview LinRegPreds
print("Preview of the predictions of the interact linear regression model on mean dominance per IAPS data:")
LinRegInteractPreds_dominance %>%
  select(linRegInteractPrediction, Actual) %>%
  head(20)

#Preview Linear Regression Model Metrics
print("Model Fit Statistics for Basic Interact Linear Regression:")
linRegInteract_dominance %>% 
  tidy() %>% 
  rename(Term = term,
         Estimate = estimate,
         `Std Error` = std.error,
         Statistic = statistic,
         `P Value` = p.value)

print("Model Fit Statistics for Basic Interact Linear Regression PT. 2:")
linRegInteract_dominance %>%
  glance() %>%
  rename(`R Squared` = r.squared,
         `Adj. R Squared` = adj.r.squared,
         Sigma = sigma,
         Statistic = statistic,
         `P Value` = p.value,
         DF = df,
         Deviance = deviance,
         `DF Residual` = df.residual)

print("Model Fit Statistics for Basic Interact Linear Regression PT. 3:")
metrics(data = LinRegInteractPreds_dominance, truth = Actual, estimate = linRegInteractPrediction) %>% 
  select(-.estimator) %>%
  rename(Metric = .metric,
         Estimate = .estimate)

```

As with the regular linear regression, it's interesting to note that the $R^2$ *are* slightly different in the above tables. I believe this is because the first one is passed to the `glance` function and describes the $R^2$ term relative to the training data, and the latter is relative to the testing data.

#### Compare Model Metrics

Below, we are simply comparing the performance metrics (mean absolute error (MAE), root mean squared error (RMSE), and r-squared ($R^2$)) for each model.

```{r compare metrics - dominance}

#Bind the metrics from each model together, add a model identification column.
#Rename the output and make it look nice.

metricComparison_dominance <- bind_rows(metrics(data = Preds_dominance, 
                                      truth = Actual, 
                                      estimate = Prediction) %>% mutate(Model = "Elastic Net"),
                              metrics(data = InteractPreds_dominance, 
                                      truth = Actual, 
                                      estimate = interactPrediction) %>% mutate(Model = "Interact, Elastic Net"),
                              metrics(data = LinRegPreds_dominance, 
                                      truth = Actual, 
                                      estimate = linRegPrediction) %>% mutate(Model = "Linear Regression"),
                              metrics(data = LinRegInteractPreds_dominance, 
                                      truth = Actual, 
                                      estimate = linRegInteractPrediction) %>% mutate(Model = "Interact Linear Regression")) %>% 
  rename(Metric = .metric,
         Estimate = .estimate) %>%
  select(-.estimator) %>%
  arrange(Metric, Estimate)

print("Comparison of Model Metrics:")
metricComparison_dominance
  
```

```{r pull RMSE metrics - dominance, include = FALSE}

#Filter the metricComparison for just the RMSE
RMSEMetrics_dominance <- 
  metricComparison_dominance %>%
  filter(Metric == "rmse")

```

#### Results

We can see from the comparison table above that the `r RMSEMetrics_dominance %>% slice(1) %>% pull(Model)` model performs best. It has a testing RMSE of `r RMSEMetrics_dominance %>% slice(1) %>% pull(Estimate) %>% round(3)`, relative to the, `r RMSEMetrics_dominance %>% slice(2) %>% pull(Model)`, which has an RMSE of `r RMSEMetrics_dominance %>% slice(2) %>% pull(Estimate) %>% round(3)`; `r RMSEMetrics_dominance %>% slice(3) %>% pull(Model)`, which has an RMSE of `r RMSEMetrics_dominance %>% slice(3) %>% pull(Estimate) %>% round(3)`; and `r RMSEMetrics_dominance %>% slice(4) %>% pull(Model)`, which has an RMSE of `r RMSEMetrics_dominance %>% slice(4) %>% pull(Estimate) %>% round(3)`.

```{r compare preds - dominance, include = FALSE}

## Compare Model Predictions
# Below, we can see the predictions of each model fit and compare them to the Actual mean valence.


#join the different models' predictions by dom1mn and reorder the columns
comparePreds_dominance <- full_join(Preds_dominance, InteractPreds_dominance, by = "Actual") %>% 
  full_join(LinRegPreds_dominance, by = "Actual") %>% 
  select(Actual, everything())

#preview comparePreds
print("Preview of the comparison in predictions of different model types:")
comparePreds_dominance %>% 
  head(10)

```

## Metric Comparison - All Outcomes

### What's the best model?

As seen in the section Modeling the Data, different models perform best depending on which outcome we are trying to predict. The `r RMSEMetrics %>% slice(1) %>% pull(Model)` predicts the valence the best; the `r RMSEMetrics_arousal %>% slice(1) %>% pull(Model)` predicts the arousal the best; and the `r RMSEMetrics_dominance %>% slice(1) %>% pull(Model)` predicts the dominance the best. 


```{r compare all model metric stuff}

bind_rows(RMSEMetrics %>% mutate(Outcome = "Valence") %>% slice(1),
          RMSEMetrics_arousal %>% mutate(Outcome = "Arousal") %>% slice(1),
          RMSEMetrics_dominance %>% mutate(Outcome = "Dominance") %>% slice(1))


```

### How do the predictions correlate?

In order to check how accurate our model is, we can plot the actual vs. predicted outcomes and observe the trends. We do this only for the best fitting model as outlined in the above section. To do so, we take the mean prediction for each image.


```{r actual vs predicted data frame, warning = FALSE, message = FALSE}

#create a dataframe that contains the average prediction for each IAPS rating
#for the best performing models given the idfferent predictor outcomes.

#First, we must join the relevant data from InteractPreds_dominance with the
#full dataset. This is because it was fit in tidymodels and doesn't have the
#full dataset unlike the LinRegPreds and LinRegInteractPreds_arousal.

joinedInteractPreds_dominance <- inner_join(InteractPreds_dominance, mutate(test, Actual = dom1mn), by = "Actual")

bestPredActual <- bind_rows(LinRegPreds %>%
  group_by(Actual) %>% 
  mutate(Predicted = mean(linRegPrediction),
         Outcome = "Valence",
         SD = valsd),
LinRegInteractPreds_arousal %>%
  group_by(Actual) %>%
  mutate(Predicted = mean(linRegInteractPrediction),
         Outcome = "Arousal",
         SD = arosd),
joinedInteractPreds_dominance %>%
  group_by(Actual) %>%
  mutate(Predicted = mean(interactPrediction),
         Outcome = "Dominance",
         SD = dom1sd)) %>%
  ungroup()
  
```
  
```{r correlations - pred/actual}

AroCor <- bestPredActual %>% filter(Outcome == "Arousal") 
DomCor <- bestPredActual %>% filter(Outcome == "Dominance")
ValCor <- bestPredActual %>% filter(Outcome == "Valence")

```

As seen in the graph below, predicted and actual ratings have a correlation of `r round(cor(x = AroCor$Actual, y = AroCor$Predicted),3)` when the outcome is arousal, `r round(cor(x = DomCor$Actual, y = DomCor$Predicted),3)` when the outcome is dominance, and `r round(cor(x = ValCor$Actual, y = ValCor$Predicted),3)`, when the outcome is valence.


```{r actual vs predicted plot}
  
bestPredActual %>%
  ggplot(aes(x = Actual, y = Predicted, color = Outcome)) + 
  geom_point(show.legend = FALSE) + 
  facet_wrap(~Outcome) +
  labs(title = "Predicted IAPS Rating vs Actual Rating") +
  myGGTheme

```

### Do the prediction errors vary with rating uncertainty?

Another question to ask is whether or not the prediction errors (the difference between actual and predicted rating) varies with the uncertainty, as measured by the standard deviation, of each rating from the IAPS dataset.

```{r prediction errors}

#create a prediction error data frame that has the difference between the
#predicted and actual outcomes. 
PE <- bestPredActual %>% 
  mutate(PE = Predicted - Actual) %>%
  select(PE, SD, Outcome)

```

```{r correlations PE/SD}

AroCorPE <- PE %>% filter(Outcome == "Arousal") 
DomCorPE <- PE %>% filter(Outcome == "Dominance")
ValCorPE <- PE %>% filter(Outcome == "Valence")

```

As seen in the graph below, prediction errors and image rating uncertainty (standard deviation) have correlations of `r round(cor(x = AroCorPE$PE, y = AroCorPE$SD),3)` when the outcome is arousal, `r round(cor(x = DomCorPE$PE, y = DomCorPE$SD),3)` when the outcome is dominance, and `r round(cor(x = ValCorPE$PE, y = ValCorPE$SD),3)`, when the outcome is valence.

```{r prediction error plots}

#Plot!
PE %>%
  ggplot(aes(x = SD, y = PE, color = Outcome)) +
  geom_point(show.legend = FALSE) +
  myGGTheme + 
  labs(y = "Prediction Error", 
       x = "Standard Deviation",
       title = "Prediction Error and Uncertainty in Image Ratings") + 
  facet_wrap(~Outcome)

```

