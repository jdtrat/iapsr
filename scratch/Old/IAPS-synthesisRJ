---
title: "IAPS Synthesis Document"
author: "Jonathan Trattner, Rachel Jones, and Delaney Teceno"
date: "Last compiled on `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    df_print: paged
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: true
      smooth_scroll: true
    theme: journal
    highlight: monochrome
    code_folding: hide
params:
  numSubjects: 6
---


```{r setup, include=FALSE}
#Load packages
library(tidyverse)
library(tidymodels)
library(patchwork)
library(janitor)
library(fs)
library(tictoc)
library(vip)
library(caret)
library(iapsr) #our package, woohoo!

myGGTheme <- ggplot2::theme(plot.title = ggplot2::element_text(hjust=0.5, face = "bold.italic", size=16), #plot title aesthetics
                   plot.subtitle = ggplot2::element_text(hjust = 0.5, face = "bold", size = 12), #plot subtitle aesthetics
                   axis.title.x = ggplot2::element_text(size = 12, color= "black", face = "bold"), #x axis title aesthetics
                   axis.title.y = ggplot2::element_text(size = 12, color= "black", face = "bold"), #y axis title aesthetics
                   axis.text.x = ggplot2::element_text(angle = 0, hjust = 0.5, size = 12),
                   #legend aesthetics
                   legend.title = ggplot2::element_text(size= 14,
                                    color = "black",
                                    face = "bold"),
                   legend.title.align = 0.5,
                   legend.text = ggplot2::element_text(size = 10,
                                              color = "black",
                                              face = "bold"),
                   legend.text.align = 0)
```

## Introduction

This document synthesizes different experiments aimed at understanding the neuro-computational basis of subjective feelings. In 2018, forty-eight subjects (original cohort) rated 120 images from the International Affective Picture System database which provides normative ratings on valence, arousal, and dominance. Our goal is to predict these normative ratings based on subjects' responses to the questions "How positive does this image make you feel?" and "How negative does this image make you feel?" We test our model on data collected during the summer of 2020 (new cohort), consisting of 24 healthy volunteers (18 females and 6 males). Subjects were recruited for this study at the Piedmont Triad Research Community Center and are between the ages of 18 and 64. We conducted an a priori power analysis using a power value of no lower than 80% for a linear regression with an α value of 0.05. We used the R package `pwr` to calculate the minimal sample size for general linear model. This suggested 20 participants would ensure 80% statistical power. All subjects were screened 24 hours in advance, and minutes before, scheduled appointments to ensure no COVID-19 symptomology. The average age of participants was “X” years old (SD = x, range = x). The study was approved by the institutional review board of Wake Forest University School of Medicine.

The first part of this document provides reproducible code for each step in the data analysis pipeline, including reading the data, tidying the data, running regression models, and visualizing results for the ratings task described above. The second part of this document provides reproducible code for analyzing performance on an IAPS choice task completed only by new cohort subjects.

## Ratings Task

### Read in Data

Below is the code to read in the data. The directories chunk is specific to the files' location on our computer and will need to be changed if run on another machine. The data is manipulated as it's read in -- please see the code comments for more detail.

```{r set directories}

dirIAPS <- '~/Desktop/IAPS/IAPSdata/' #set the directory for IAPS data
dirData <- '~/Desktop/IAPS/Subject Data/' #set the directory for subject data
dirImageOrder <- '~/Desktop/IAPS/Subject Image Order/' #set the directory for image order
dirRatings <- '~/Desktop/IAPS/RJT/ratings/' #for new cohort
fileRatings <- dir_ls(dirRatings, glob = "*.txt.txt") #get the paths for the files that contain the string ".txt.txt". For new cohort.
filesIAPS <- dir_ls(dirIAPS) #list the files in the IAPS directory
filesData <- dir_ls(dirData) #list the files in the subject data directory
filesImageOrder <- dir_ls(dirImageOrder) #list the files in the image order directory

```

```{r read data, message = FALSE, warning = FALSE}


#Read in the demographics data
demographics <- read_csv(file = "C:/Users/rachjone/Desktop/IAPS/Demographics/KLRIF_2018-07-09.csv")

#Import the IAPS key data set
keyIAPS <- filesIAPS %>%
  map_df(~read_csv(.x, na = c("", ".", NA),
                   col_types = cols(IAPS = col_character())) %>%
           select(desc:dom1sd) %>%
           drop_na(),
         .id = "form") %>%
  mutate(form = case_when(str_detect(form, "1_IAPS") ~ 1,
                          str_detect(form, "2_IAPS") ~ 2,
                          str_detect(form, "3_IAPS") ~ 3,
                          str_detect(form, "4_IAPS") ~ 4),
         form = as.factor(form))

#Read in subject rating
subjectData <- filesData %>%
  map_df(~read_csv(.x) %>%
           rename(time = Timestamp,
                  subject = `Subject ID:`),
         .id = "form") %>%
  mutate(form = case_when(str_detect(form, "1_SFA") ~ 1,
                          str_detect(form, "2_SFA") ~ 2,
                          str_detect(form, "3_SFA") ~ 3,
                          str_detect(form, "4_SFA") ~ 4),
           form = as.factor(form)) %>%
  #pivot the data so all ratings are in a column "rating" instead of in their own rows.
  pivot_longer(cols = -c(form, time, subject), values_to = "rating") %>%
  select(-name) %>%
  #have a running order for each subject
  group_by(subject) %>%
  mutate(order = row_number()) %>%
  ungroup() %>%
  #make this subjectID string match that of imageOrder
  mutate(subject = str_replace(subject, "\\-", "\\_")) %>%
  select(-time) %>%
  #arrange by each subject and the form (so one subject goes through all four
  #forms before moving to next subject)
  arrange(subject, form)


  imageOrder <- filesImageOrder %>%
  map_df(~read_csv(.x),
         .id = "form") %>%
  mutate(form = case_when(str_detect(form, "1_pictureID") ~ 1,
                          str_detect(form, "2_pictureID") ~ 2,
                          str_detect(form, "3_pictureID") ~ 3,
                          str_detect(form, "4_pictureID") ~ 4),
         form = as.factor(form)) %>%
  #pivot the data so all image values (IDs) are in a column "value"
  pivot_longer(cols = -c(X1, form),
               names_to = "subject") %>%
  #remvoe subject prefix
  mutate(subject = str_remove(.$subject, "\\d\\_")) %>%
  #arrange by subject
  arrange(subject) %>%
  #remove NAs that appear because of how the data was read in where each form's
  #image ID were basically in a new column for the subject with the prefix
  #1_SFA_..."
  drop_na() %>%
  #have a running order for each subject, and remove the X1 column.
  group_by(subject) %>%
  mutate(order = row_number()) %>%
  ungroup() %>%
  select(-X1)

#Read in and manipulate the new cohort data.
newCohortRatings <-
  #Read in all the ratings data into a list with each element corresponding to a
  #different file.
  map(fileRatings, ~readRatings(.x)) %>%
  #take each list element (subject file) and call our function
  #processRatingsData, creating an ID column called "subject". This column
  #simply contains the file path string name.
  map_df(~processRatingsData(.x), .id = "subject") %>%
  #extract from the subject column the IDs. For example "IAPS_B_01".
  mutate(subject = str_extract(subject, "[:graph:]{9}(?=rating.txt)"))

#Read in and manipulate the new cohort demographics information. File path is
#dependent on user's computer.
newCohortDems <- read_csv(file = 'C:/Users/rachjone/Desktop/IAPS/RJT/demographics/IAPSdems.csv') %>%
  mutate(gender = case_when(gender == "female" ~ 0,
                            gender == "male" ~ 1))

```

### Tidy the Data

This section describes how we tidied the data in order to get it in a format to perform the elastic net regression. Please see code comments for more details.

```{r tidy data}

#tidy the demographics data
tidyDems <- demographics %>%
  #rename variables
  rename(subject = "Subject ID",
         age = "Age (years)",
         gender = "What is your gender?") %>%
  #make subject characters line up with others and recode gender as binary for
  #regression purposes
  mutate(subject = str_replace(subject, "\\-", "\\_"),
         gender = recode(gender, "Male" = 1, "Female" = 0)) %>%
  #select only relevant variables
  select(subject, age, gender)


#join the imageOrder and subject data by subject, form, and order.
orderedSubjectData <- full_join(imageOrder, subjectData, by = c("form", "subject", "order")) %>%
  rename(picID = value)

#Label the orderedSubjectData with a column that has the type of question asked
#based on the picID's P or N. Then rename the picID column as IAPS, removing any
#"P" or "N" at the end, so the names are the same as in the keyIAPS dataframe.
labeledOrderedSubjectData <- orderedSubjectData %>%
  mutate(question = case_when(str_detect(picID, "P") ~ "positive",
                              str_detect(picID, "N") ~ "negative",
                              TRUE ~ "Oops"),
         picID = str_remove_all(picID, "P|N")) %>%
  rename(IAPS = picID)

#create positiveRatings as a dataframe that has IAPS data from the rounds where
#it was asked how positive the image made them feel. Then combine the rating and
#question data into one column that has the ratings for when the question was
#about positive feelings. I could've done this using the following code instead.
#Same thing.

#labeledOrderedSubjectData %>%
#  filter(question == "positive") %>%
#  select(-question) %>%
#  rename(positive = rating))

positiveRatings <- labeledOrderedSubjectData %>%
  filter(question == "positive") %>%
  pivot_wider(names_from = question, values_from = rating)

#create negativeRatings as a dataframe that has IAPS data from the rounds where
#it was asked how negative the image made them feel. Then combine the
#rating and question data into one column that has the ratings for when the
#question was about negative feelings.
negativeRatings <- labeledOrderedSubjectData %>%
  filter(question == "negative") %>%
  pivot_wider(names_from = question, values_from = rating)

#Combine the positive and negative ratings data by IAPS, subject, and form.
#Could've just used IAPS but just more specificity. Then remove miscellaneous
#order column and join that with the IAPS key. Then join that with the
#demographics info. Then remove NAs since there are some images participants saw
#that are not standardized in the IAPS key (and one participant who has no
#gender data and we exclude). The final data has 14 columns and 5640 rows and
#consists of 120 positive and negative ratings from 47 subjects.
finalData <- full_join(positiveRatings, negativeRatings, by = c("IAPS", "subject", "form")) %>%
  select(-c(order.x, order.y)) %>%
  full_join(keyIAPS, by = c("IAPS", "form")) %>%
  full_join(tidyDems, by = "subject") %>%
  drop_na() %>%
  select(subject, age, gender,
         form, IAPS, desc,
         positive, negative, everything())

# Create the final data to perform regressions by setting up the new cohort
# ratings data using the iapsr regSetup function, joining that with the new
# cohort demographics information by subject, and joining that by the IAPS
# ratings key.
newCohortFinalData <- newCohortRatings %>%
  #remove a picture not in the IAPS key
  filter(picture != "7037") %>%
  regSetup() %>%
  left_join(newCohortDems, by = "subject") %>%
  left_join(keyIAPS, by = "IAPS")

```


```{r read data, message=FALSE, warning=FALSE}
#Import the IAPS key data set
keyIAPS <- filesIAPS %>%
  map_df(~read_csv(.x, na = c("", ".", NA),
                   col_types = cols(IAPS = col_character())) %>%
           select(desc:dom1sd) %>%
           drop_na(),
         .id = "form") %>%
  mutate(form = case_when(str_detect(form, "1_IAPS") ~ 1,
                          str_detect(form, "2_IAPS") ~ 2,
                          str_detect(form, "3_IAPS") ~ 3,
                          str_detect(form, "4_IAPS") ~ 4),
         form = as.factor(form))

#Read in subject rating
subjectData <- filesData %>%
  map_df(~read_csv(.x) %>%
           rename(time = Timestamp,
                  subject = `Subject ID:`),
         .id = "form") %>%
  mutate(form = case_when(str_detect(form, "1_SFA") ~ 1,
                          str_detect(form, "2_SFA") ~ 2,
                          str_detect(form, "3_SFA") ~ 3,
                          str_detect(form, "4_SFA") ~ 4),
           form = as.factor(form)) %>%
  #pivot the data so all ratings are in a column "rating" instead of in their own rows.
  pivot_longer(cols = -c(form, time, subject), values_to = "rating") %>%
  select(-name) %>%
  #have a running order for each subject
  group_by(subject) %>%
  mutate(order = row_number()) %>%
  ungroup() %>%
  #make this subjectID string match that of imageOrder
  mutate(subject = str_replace(subject, "\\-", "\\_")) %>%
  select(-time) %>%
  #arrange by each subject and the form (so one subject goes through all four
  #forms before moving to next subject)
  arrange(subject, form)


  imageOrder <- filesImageOrder %>%
  map_df(~read_csv(.x),
         .id = "form") %>%
  mutate(form = case_when(str_detect(form, "1_pictureID") ~ 1,
                          str_detect(form, "2_pictureID") ~ 2,
                          str_detect(form, "3_pictureID") ~ 3,
                          str_detect(form, "4_pictureID") ~ 4),
         form = as.factor(form)) %>%
  #pivot the data so all image values (IDs) are in a column "value"
  pivot_longer(cols = -c(X1, form),
               names_to = "subject") %>%
  #remvoe subject prefix
  mutate(subject = str_remove(.$subject, "\\d\\_")) %>%
  #arrange by subject
  arrange(subject) %>%
  #remove NAs that appear because of how the data was read in where each form's
  #image ID were basically in a new column for the subject with the prefix
  #1_SFA_..."
  drop_na() %>%
  #have a running order for each subject, and remove the X1 column.
  group_by(subject) %>%
  mutate(order = row_number()) %>%
  ungroup() %>%
  select(-X1)

#read in demographics
demographics <- read_csv(file = 'C:/Users/rachjone/Desktop/IAPS/Demographics/KLRIF_2018-07-09.csv')

#tidy the demographics data
tidyDems <- demographics %>%
  rename(subject = "Subject ID",
         age = "Age (years)",
         gender = "What is your gender?") %>%
  mutate(subject = str_replace(subject, "\\-", "\\_"),
         gender = recode(gender, "Male" = 1, "Female" = 0)) %>%
  select(subject, age, gender)


#join the imageOrder and subject data by subject, form, and order.
orderedSubjectData <- full_join(imageOrder, subjectData, by = c("form", "subject", "order")) %>%
  rename(picID = value)

labeledOrderedSubjectData <- orderedSubjectData %>%
  mutate(question = case_when(str_detect(picID, "P") ~ "positive",
                              str_detect(picID, "N") ~ "negative",
                              TRUE ~ "Oops"),
         picID = str_remove_all(picID, "P|N")) %>%
  rename(IAPS = picID)

positiveRatings <- labeledOrderedSubjectData %>%
  filter(question == "positive") %>%
  pivot_wider(names_from = question, values_from = rating)

negativeRatings <- labeledOrderedSubjectData %>%
  filter(question == "negative") %>%
  pivot_wider(names_from = question, values_from = rating)
#Read raw demographic data that includes 84 subjects.
Dems <-read_csv(file = 'C:/Users/rachjone/Desktop/IAPS/SFA_demographics.csv')

#Tidy the data and filter to only display demographic variables of interest (age, gender, race, ethnicity, history of psychiatric condition and medications, and current drug, alcohol, and smoking use.)
tidyDems2 <- Dems %>%
    clean_names %>%
    select(participant_id, gender, race_1:race_6, age, psychiatric_condition, condition_description, smoke_current, alcohol_current, drugs_current, current_use_description) %>%
  pivot_longer(cols = c(race_1:race_6), values_to = "race") %>%
  rename(subject = participant_id) %>%
  mutate(name = recode(name, race_1 = "Asian", race_2 = "NHPI", race_3 = "AIAN", race_4 = "Black", race_5 = "White", race_6 = "Other"),
  gender = recode(gender, "1" = "0", "2" = "1"),
  current_use_description = recode(current_use_description, "marijuana" = "Marijuana"),
  subject = str_replace(subject, "\\-", "\\_")) %>%
  filter(race == "1" | subject == "SFA_1034" | subject == "SFA_1039" | subject == "SFA_1049" | subject == "SFA_1071") %>%
  distinct (subject, race, .keep_all = TRUE)

#Join table with the 47 subjects we used for our analyses. This gets rid of extra subjects we did not include for the old cohort (There were 84 total, and we only used 47 in our analyse).
finalData <- full_join(positiveRatings, negativeRatings, by = c("IAPS", "subject", "form")) %>%
  select(-c(order.x, order.y)) %>%
  full_join(keyIAPS, by = c("IAPS", "form")) %>%
  full_join(tidyDems, by = "subject") %>%
  drop_na() %>%
  select(subject, age, gender,
         form, IAPS, desc,
         positive, negative, everything())

finalData2 <- finalData %>%
 select(-c(age:dom1sd)) %>%
 distinct()

DemJoin <- right_join(tidyDems2, finalData2, by = "subject")

DemJoin$gender <- as.numeric(DemJoin$gender)
DemJoin$smoke_current <- as.numeric(DemJoin$smoke_current)
DemJoin$alcohol_current <- as.numeric(DemJoin$alcohol_current)
DemJoin$drugs_current <- as.numeric(DemJoin$drugs_current)

#Create output table.
dem.tab <- DemJoin %>%
mutate(name=ifelse(race == "0",NA,name),
  condition_description = recode(condition_description, "Generalized Anxiety Disorder" = "Anxiety", "depression, anxiety, eating disorder" = "Depression & Anxiety", "Depression, Bulimia Nervosa" = "Depression &  Bulimia Nervosa")) %>%
  select(-race) %>%
  rename(race = name)

head(dem.tab)
```

Below is a summary of the demographics variables from our original 47-subject IAPS cohort (2018).
```{r asdf, warning = FALSE}

myGGTheme <- ggplot2::theme(plot.title = ggplot2::element_text(hjust=0.5, face = "bold.italic", size=16), #plot title aesthetics
                   plot.subtitle = ggplot2::element_text(hjust = 0.5, face = "bold", size = 12), #plot subtitle aesthetics
                   axis.title.x = ggplot2::element_text(size = 12, color= "black", face = "bold"), #x axis title aesthetics
                   axis.title.y = ggplot2::element_text(size = 12, color= "black", face = "bold"), #y axis title aesthetics
                   axis.text.x = ggplot2::element_text(angle = 0, hjust = 0.5, size = 12),
                   #legend aesthetics
                   legend.title = ggplot2::element_text(size= 14,
                                    color = "black",
                                    face = "bold"),
                   legend.title.align = 0.5,
                   legend.text = ggplot2::element_text(size = 10,
                                              color = "black",
                                              face = "bold"),
                   legend.text.align = 0)

raceCount <- dem.tab %>% count(race) %>%
  drop_na() %>%
  mutate(race = fct_reorder(race, desc(n))) %>%
  ggplot(aes(x = race, y = n, fill = race)) +
  geom_col() +
  labs(y = "Count",
       x = "Race",
       #caption = "Four subjects were removed because they ommitted their race.",
       title = "Subject Race Demographic",
       subtitle = "Original Cohort") +
  myGGTheme +
  scale_x_discrete(labels = c("White", "Black", "Asian", "Other")) +
  theme(legend.position = "none",
        panel.background = element_rect(fill = "aliceblue"),
        plot.background = element_rect(fill = "aliceblue"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_line(color = "lightblue1"))

ageCount <- dem.tab %>% select(age) %>%
  ggplot(aes(x = age, fill = "red")) +
  geom_histogram(binwidth = 2) +
  labs(y = "Count",
       x = "Age (years)",
       title = "Subject Age Demographic",
       subtitle = "Original Cohort") +
  myGGTheme +
  theme(legend.position = "none",
        panel.background = element_rect(fill = "aliceblue"),
        plot.background = element_rect(fill = "aliceblue"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_line(color = "lightblue1"))

genderCount <- dem.tab %>% count(gender) %>%
  ggplot(aes(x = gender, y = n, fill = gender)) +
  geom_col() +
  labs(y = "Count",
       x = "Gender",
       title = "Subject Gender Demographic",
       subtitle = "Original Cohort") +
  myGGTheme +
  scale_x_discrete(labels = c("Female", "Male")) +
  theme(legend.position = "none",
        panel.background = element_rect(fill = "aliceblue"),
        plot.background = element_rect(fill = "aliceblue"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_line(color = "lightblue1"))

psychCondtionCount <- dem.tab %>% count(condition_description) %>%
  drop_na() %>%
  mutate(condition_description = fct_reorder(condition_description, desc(n))) %>%
  ggplot(aes(x = condition_description, y = n, fill = condition_description)) +
  geom_col() +
  labs(y = "Count",
       x = "Psychiatric Condition",
       title = "Subject Psychiatric Condition Demographic",
       subtitle = "Original Cohort") +
  myGGTheme +
  theme(legend.position = "none",
        panel.background = element_rect(fill = "aliceblue"),
        plot.background = element_rect(fill = "aliceblue"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_line(color = "lightblue1"))

alcoholCount <- dem.tab %>% count(alcohol_current) %>%
  drop_na() %>%
  ggplot(aes(x = alcohol_current, y = n, fill = alcohol_current)) +
  geom_bar(stat = "identity") +
  geom_col() +
  labs(y = "Count",
       x = "Current Alcohol Users",
       title = "Subject Alcohol Use Demographic",
       subtitle = "Original Cohort") +
  myGGTheme +
  scale_x_discrete(limits= "Yes", "No") +
  theme(legend.position = "none",
        panel.background = element_rect(fill = "aliceblue"),
        plot.background = element_rect(fill = "aliceblue"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_line(color = "lightblue1"))

drugsCount <- dem.tab %>% count(drugs_current) %>%
  drop_na() %>%
  ggplot(aes(x = drugs_current, y = n, fill = drugs_current)) +
  geom_bar(stat = "identity") +
  geom_col() +
  labs(y = "Count",
       x = "Current Marijuana Users",
       title = "Subject Marijuana Use Demographic",
       subtitle = "Original Cohort") +
  myGGTheme +
  scale_x_discrete(limits= "Yes", "No") +
  theme(legend.position = "none",
        panel.background = element_rect(fill = "aliceblue"),
        plot.background = element_rect(fill = "aliceblue"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_line(color = "lightblue1"))

smokeCount <- dem.tab %>% count(smoke_current) %>%
  drop_na() %>%
  ggplot(aes(x = smoke_current, y = n, fill = smoke_current)) +
  geom_bar(stat = "identity") +
  geom_col() +
  labs(y = "Count",
       x = "Current Smokers",
       title = "Subject Smoking Demographic",
       subtitle = "Original Cohort") +
  myGGTheme +
  scale_x_discrete(labels = c("Yes", "No")) +
  theme(legend.position = "none",
        panel.background = element_rect(fill = "aliceblue"),
        plot.background = element_rect(fill = "aliceblue"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_line(color = "lightblue1"))


(raceCount | ageCount | genderCount)/ (alcoholCount | drugsCount | smokeCount)/ psychCondtionCount

```

Below, we can see a preview of the data on which we will fit regression models. The first table shows the first 10 rows for subject SFA_1025 from the original cohort. The second table shows the first 10 rows for subject IAPS_B_01. The age column is in years, gender is binary (0 for females 1 for males for use in regression analysis), IAPS column is the picture ID. The desc column is a description of the IAPS images. The positive and negative columns show the subject's ratings when asked "How positive does this image make you feel?" and "How negative does this image make you feel?". The remaining columns describe the mean and standard deviation for the image's standardized valence, arousal, and dominance, per the IAPS dataset.

```{r preview, cols.print = 7, echo = FALSE}

#preview the regression data frame for subject SFA_1025
head(finalData, 10)

#preview the regression data frame for subject IAPS_B_01
head(newCohortFinalData, 10)

```

### Modeling the Data

#### Methods and Code

Using both the `tidymodels` framework in R and the `caret` package in R, we tuned an elastic net regression with a wide array of alpha (0 to 1) and lambda (0 to 200) values using six fold and ten fold cross validation. Our response variable was the mean rating of valence, arousal, or dominance, per the IAPS dataset. Our predictors were subjects' positive and negative ratings (and the interaction between them), age, and gender. We looked at the root mean squared error (rmse), mean absolute error (mae), and the $R^2$ metrics to evaluate model fit on a dataset of `r params$numSubjects` subjects whose ratings were collected during the summer of 2020 (new cohort). These indicated that a regular linear regression performed best. We used both the `caret` package to perform 10 fold cross validation on the data, and the `lm` function in R fit on the entire dataset. The linear models performed equally well independent of cross validation. It should also be noted that we tested the linear model without the interaction between positive and negative ratings and it performed worse on all metrics. Included are code snippets for performing linear regressions using `lm` and `caret`. Elastic net regression code is included but model coefficients are not explicitly reported.

```{r model set up - interact, eval = FALSE}

#create a recipe object -- simply describing the formula to fit for predicting
#the mean valence from the positive and negative ratings. THEN add the
#interaction between positive and negative.
interactRec <- recipe(valmn ~ positive + negative + age + gender, data = train) %>%
  step_interact(terms = ~ positive:negative)


#create a workflow object with the recipe and model specification
wfInteract <- workflow() %>%
  add_recipe(interactRec) %>%
  add_model(netSpec)

```

```{r tune interact model, warning = FALSE, message = FALSE, eval = FALSE}

#tune the interact model
tic("tune interact grid")
netTunedInteract <- tune_grid(wfInteract,
                      resamples = CVtraining,
                      grid = netGrid)
toc()

```

```{r interact metric plot, warning = FALSE, metric = FALSE, eval = FALSE}

#preview penalty and mean RMSE/RSQ
netTunedInteract %>% collect_metrics() %>%
ggplot(aes(penalty, mean, color = mixture)) +
  geom_point(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  labs(y = "Mean", x = "Penalty", color = "Mixture", title = "Average Metric and Penalty") +
  myGGTheme

```


```{r interact metrics, eval = FALSE}
#preview the lowest RMSE -- best metrics that yield it
print("Penalty and mixture and their relation to RMSE - Interact model:")
netTunedInteract %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  arrange(mean) %>%
  select(-c(.estimator, n)) %>%
  rename(Penalty = penalty,
         Mixture = mixture,
         Metric = .metric,
         Mean = mean,
         `Std Error` = std_err) %>%
  head(10)


```

```{r fit best interact model, eval = FALSE}

#select best interact metrics
bestMetricsInteract <- netTunedInteract %>% select_best(metric = "rmse")

#finalize the workflow with the best interact metrics
finalWFInteract <- finalize_workflow(wf, parameters = bestMetricsInteract)

#save last fit
testFitInteract <- last_fit(finalWFInteract, split = split)

#preview testFit metrics
testFitInteract %>%
  collect_metrics() %>%
  select(-.estimator) %>%
  rename(Metric = .metric,
         Estimate = .estimate)

#Save interact model's predictions
InteractPreds <- testFitInteract %>%
  select(.predictions) %>%
  unnest(.predictions) %>%
  rename(interactPrediction = .pred,
         Actual = valmn)

#preview interact model's predictions
print("Preview of the predictions of best fitting interact model on mean valence per IAPS data:")
InteractPreds %>%
  select(interactPrediction, Actual) %>%
  head(20)


#preview model metrics
print("Model Beta Estimates for Elastic Net Regression - Interact:")
finalWFInteract %>%
  fit(test) %>%
  pull_workflow_fit() %>%
  vi(lambda = bestMetrics$penalty) %>%
  select(-Sign)

print("Interact Model Fit Metrics:")
metrics(InteractPreds, truth = Actual, estimate = interactPrediction) %>%
  select(-.estimator) %>%
  rename(Metric = .metric,
         Estimate = .estimate)

```

```{r caret interaction, warning = FALSE, message = FALSE, eval = FALSE}

#### Create a list of formulas for the outcome
outcomeFormulasCaret <- list("arousal" = aromn ~ positive + negative + (positive * negative) + age + gender,
                        "dominance" = dom1mn ~ positive + negative + (positive * negative) + age + gender,
                        "valence" = valmn ~ positive + negative + (positive * negative) + age + gender)

#Create strings vectors/lists that have the outcome strings and truths to supply in a map2 function for getting the metrics.
outcomeStrings <- list("arousal" = "arousal", "dominance" = "dominance", "valence" = "valence")
outcomeTruths <- c("aromn", "dom1mn", "valmn")

#create a train control object
careTrain <- trainControl(method = "cv", number = 10)

#set the seed and predict the data for each model using 10 fold cross validation
set.seed(18)
caretNewPredicted <- map(outcomeFormulasCaret, ~ train(.x, data = finalData, trControl = careTrain, method = "lm")) %>%
  map_df(~predict(.x, newCohortFinalData) %>% bind_cols(newCohortFinalData), .id = "outcome") %>%
  rename(predicted = `...1`)

#get the metrics for the caret model
caretNewPredMetrics <- map2_df(outcomeStrings, outcomeTruths, ~ caretNewPredicted %>% filter(outcome == .x) %>%
                                 metrics(truth = .y, estimate = predicted), .id = "outcome")

```

```{r caret - no interaction, warning = FALSE, message = FALSE, eval = FALSE}


#### Create a list of formulas for the outcome
outcomeFormulasCaretNoInteraction <- list("arousal" = aromn ~ positive + negative + age + gender,
                        "dominance" = dom1mn ~ positive + negative + age + gender,
                        "valence" = valmn ~ positive + negative + age + gender)

#Create strings vectors/lists that have the outcome strings and truths to supply in a map2 function for getting the metrics.
outcomeStrings <- list("arousal" = "arousal", "dominance" = "dominance", "valence" = "valence")
outcomeTruths <- c("aromn", "dom1mn", "valmn")

#create a train control object
careTrain <- trainControl(method = "cv", number = 10)

#set the seed and predict the data for each model using 10 fold cross validation
set.seed(18)
caretNewPredictedNoInteraction <- map(outcomeFormulasCaretNoInteraction, ~ train(.x, data = finalData, trControl = careTrain, method = "lm")) %>%
  map_df(~predict(.x, newCohortFinalData) %>% bind_cols(newCohortFinalData), .id = "outcome") %>%
  rename(predicted = `...1`)

#get the metrics for the caret model
caretNewPredMetricsNoInteraction <- map2_df(outcomeStrings, outcomeTruths, ~ caretNewPredictedNoInteraction %>% filter(outcome == .x) %>%
                                 metrics(truth = .y, estimate = predicted), .id = "outcome")


```

```{r compare carets, eval = FALSE}

compareCarets <- bind_rows(caretNewPredMetricsNoInteraction %>% mutate(type = "no interaction"),
                        caretNewPredMetrics %>% mutate(type = "interaction"))

```

```{r lm - no interaction, warning = FALSE, message = FALSE}

outcomeFormulasNoInteraction <- list("arousal" = lm(formula = aromn ~ positive + negative + age + gender, data = finalData),
                                     "dominance" = lm(formula = dom1mn ~ positive + negative + age + gender, data = finalData),
                                     "valence" = lm(formula = valmn ~ positive + negative + age + gender, data = finalData))


#Create strings vectors/lists that have the outcome strings and truths to supply in a map2 function for getting the metrics.
outcomeStrings <- list("arousal" = "arousal", "dominance" = "dominance", "valence" = "valence")
outcomeTruths <- c("aromn", "dom1mn", "valmn")


set.seed(18)
# Create a dataframe with the predictions for each based on the outcome
newCohortPredictedNoInteraction <- map_df(outcomeFormulasNoInteraction, ~ predict(.x, newCohortFinalData) %>% bind_cols(newCohortFinalData), .id = "outcome") %>%
  rename(predicted = `...1`)

#get metrics for each model's fit on the new data
newCohortPredictedMetricsNoInteraction <- map2_df(outcomeStrings, outcomeTruths, ~ newCohortPredictedNoInteraction %>% filter(outcome == .x) %>% metrics(truth = .y, estimate = predicted), .id = "outcome")


```

```{r lm - interaction, warning = FALSE, message = FALSE}

#### Create a list of formulas for the outcome
outcomeFormulas <- list("arousal" = lm(formula = aromn ~ positive + negative + (positive * negative) + age + gender, data = finalData),
                        "dominance" = lm(formula = dom1mn ~ positive + negative + (positive * negative) + age + gender, data = finalData),
                        "valence" = lm(formula = valmn ~ positive + negative + (positive * negative) + age + gender, data = finalData))


set.seed(18)
# Create a dataframe with the predictions for each based on the outcome
newCohortPredicted <- map_df(outcomeFormulas, ~ predict(.x, newCohortFinalData) %>% bind_cols(newCohortFinalData), .id = "outcome") %>%
  rename(predicted = `...1`)

#get metrics for each model's fit on the new data
newCohortPredictedMetrics <- map2_df(outcomeStrings, outcomeTruths, ~ newCohortPredicted %>% filter(outcome == .x) %>%
                                       metrics(truth = .y, estimate = predicted), .id = "outcome")

```

```{r compare lms}

compareLMs <- bind_rows(newCohortPredictedMetricsNoInteraction %>% mutate(type = "no interaction"),
                        newCohortPredictedMetrics %>% mutate(type = "interaction"))

```

```{r caret glmnet, eval = FALSE}

 #### Create a list of formulas for the outcome
 outcomeFormulasCaret <- list("arousal" = aromn ~ positive + negative + (positive * negative) + age + gender,
                              "dominance" = dom1mn ~ positive + negative + (positive * negative) + age + gender,
                              "valence" = valmn ~ positive + negative + (positive * negative) + age + gender)

 #Create strings vectors/lists that have the outcome strings and truths to supply in a map2 function for getting the metrics.
 outcomeStrings <- list("arousal" = "arousal", "dominance" = "dominance", "valence" = "valence")
 outcomeTruths <- c("aromn", "dom1mn", "valmn")

 #create a train control object
 careTrain <- trainControl(method = "cv", number = 10)
 caretGrid <- expand.grid(alpha = seq(0, 1, by = 0.1),
   lambda = c(seq(0, 1, length = 20), seq(2,10, by = 1), seq(15,200, by = 5)))

 #set the seed and predict the data for each model using 10 fold cross validation
 set.seed(18)
 caretGlmnetNewPredicted <- map(outcomeFormulasCaret, ~ train(.x, data = finalData, method = "glmnet", trControl = careTrain, tuneGrid = caretGrid)) %>%
   map_df(~predict(.x, newCohortFinalData) %>% bind_cols(newCohortFinalData), .id = "outcome") %>%
   rename(predicted = `...1`)

```

```{r caret glmnet compare, eval = FALSE}

 #get the metrics for the caret model
 caretGlmnetNewPredMetrics <- map2_df(outcomeStrings, outcomeTruths, ~ caretGlmnetNewPredicted %>% filter(outcome == .x) %>%
                                  metrics(truth = .y, estimate = predicted), .id = "outcome")

```


```{r caret glmnet - no interaction, eval = FALSE}

 #### Create a list of formulas for the outcome
 outcomeFormulasCaretNoInteraction <- list("arousal" = aromn ~ positive + negative + age + gender,
                              "dominance" = dom1mn ~ positive + negative + age + gender,
                              "valence" = valmn ~ positive + negative + age + gender)

 #Create strings vectors/lists that have the outcome strings and truths to supply in a map2 function for getting the metrics.
 outcomeStrings <- list("arousal" = "arousal", "dominance" = "dominance", "valence" = "valence")
 outcomeTruths <- c("aromn", "dom1mn", "valmn")

 #create a train control object
 careTrain <- trainControl(method = "cv", number = 10)
 caretGrid <- expand.grid(alpha = seq(0, 1, by = 0.1),
                          lambda = c(seq(0, 1, length = 20), seq(2,10, by = 1), seq(15,200, by = 5)))

 #set the seed and predict the data for each model using 10 fold cross validation
 set.seed(18)
 caretGlmnetNewPredNoInteraction <- map(outcomeFormulasCaretNoInteraction, ~ train(.x, data = finalData, method = "glmnet", trControl = careTrain, tuneGrid = caretGrid)) %>%
   map_df(~predict(.x, newCohortFinalData) %>% bind_cols(newCohortFinalData), .id = "outcome") %>%
   rename(predicted = `...1`)

 #get the metrics for the caret model
 caretGlmnetNewPredMetricsNoInteraction <- map2_df(outcomeStrings, outcomeTruths, ~ caretGlmnetNewPredNoInteraction %>% filter(outcome == .x) %>%
                                  metrics(truth = .y, estimate = predicted), .id = "outcome")

```

```{r caret glmnet compare - no interaction, eval = FALSE}

 #### compare caret glmnets
 compareCaretsGlmnet <- bind_rows(caretGlmnetNewPredMetricsNoInteraction %>% mutate(type = "no interaction"),
                            caretGlmnetNewPredMetrics %>% mutate(type = "interaction"))

```

#### Performance Metrics

To reiterate, our best model is a simple linear regression that predicts the mean arousal, dominance, and valence level using subjects' positive and negative ratings (and the interaction between them), age, and gender. The model statistics can be seen in the tables below.

```{r lm - interaction metrics}

print("Linear Regression Coefficients for Predicting Arousal:")
outcomeFormulas$arousal %>% tidy()

print("Linear Regression Coefficients for Predicting Dominance:")
outcomeFormulas$dominance %>% tidy()

print("Linear Regression Coefficients for Predicting Valence:")
outcomeFormulas$valence %>% tidy()

print("Linear Regression Prediction Metrics:")
newCohortPredictedMetrics

```

#### How do the predictions correlate?

```{r actual vs predicted data frame, warning = FALSE, message = FALSE}

#The actual column is "rating_mean" and the predicted column is "predicted"
actual_predicted <- newCohortPredicted %>%
  pivot_longer(cols = contains("mn"), names_to = "mean_type", values_to = "rating_mean") %>%
  pivot_longer(cols = contains("sd"), names_to = "sd_type", values_to = "rating_sd") %>%
  filter((outcome == "arousal" & mean_type == "aromn" & sd_type == "arosd") |
           (outcome == "dominance" & mean_type == "dom1mn" & sd_type == "dom1sd") |
           (outcome == "valence" & mean_type == "valmn" & sd_type == "valsd")) %>%
  mutate(outcome = str_to_sentence(outcome))

```

```{r correlations - pred/actual}

AroCor <- actual_predicted %>% filter(outcome == "Arousal")
DomCor <- actual_predicted %>% filter(outcome == "Dominance")
ValCor <- actual_predicted %>% filter(outcome == "Valence")

```

In order to check how accurate our model is, we can plot the actual vs. predicted outcomes and observe the trends. We do this only for the best fitting model as outlined in the above section. The first plot is an average over all `r params$numSubjects` subjects in the new cohort. The predicted and actual ratings have an overall correlation of `r round(cor(x = AroCor$rating_mean, y = AroCor$predicted),3)` when the outcome is arousal, `r round(cor(x = DomCor$rating_mean, y = DomCor$predicted),3)` when the outcome is dominance, and `r round(cor(x = ValCor$rating_mean, y = ValCor$predicted),3)`, when the outcome is valence. The second plot shows the individual variations in these relationships.

```{r actual vs predicted plot, warning = FALSE, message = FALSE}

#On the whole
actual_predicted %>%
  ggplot(aes(x = rating_mean, y = predicted, color = outcome)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~outcome) +
  scale_y_continuous(limits = c(1,9), breaks = c(seq(1,9))) +
  scale_x_continuous(limits = c(1,9), breaks = c(seq(1,9))) +
  geom_smooth(method = "lm", show.legend = FALSE, color = "black", linetype = "dashed") +
  labs(title = "Predicted IAPS Rating vs Actual Rating",
       y = "Predicted Rating",
       x = "Actual Rating") +
  myGGTheme +
  theme(plot.background = element_rect(fill = "aliceblue"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_line(color = "lightblue1"),
        panel.background = element_rect(fill = "aliceblue"),
        strip.background = element_rect(fill = "lightblue"),
        strip.text = element_text(face = "bold"))

```



```{r actual vs predicted by subject plot, warning = FALSE, message = FALSE}

#On the whole
actual_predicted %>%
  ggplot(aes(x = rating_mean, y = predicted, color = outcome)) +
  geom_point(show.legend = FALSE) +
  facet_grid(outcome ~ subject, switch = "y") +
  scale_y_continuous(limits = c(1,9), breaks = c(seq(1,9))) +
  scale_x_continuous(limits = c(1,9), breaks = c(seq(1,9))) +
  geom_smooth(method = "lm", show.legend = FALSE, color = "black", linetype = "dashed") +
  labs(title = "Predicted IAPS Rating vs Actual Rating",
       subtitle = "Faceted by Every Subject in the New Cohort",
       y = "Predicted Rating",
       x = "Actual Rating") +
  myGGTheme +
  theme(plot.background = element_rect(fill = "aliceblue"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_line(color = "lightblue1"),
        panel.background = element_rect(fill = "aliceblue"),
        strip.background = element_rect(fill = "lightblue"),
        strip.text = element_text(face = "bold"))

```



#### Do the prediction errors vary with rating uncertainty?

Another question to ask is whether or not the prediction errors (the difference between actual and predicted rating) varies with the uncertainty, as measured by the standard deviation, of each rating from the IAPS dataset.

```{r prediction errors}

#create a prediction error data frame that has the difference between the
#predicted and actual outcomes.
PE <- actual_predicted %>%
  mutate(PE = predicted - rating_mean,
         PE = abs(PE)) %>%
  select(PE, rating_sd, outcome)

```

```{r correlations PE/SD}

AroCorPE <- PE %>% filter(outcome == "Arousal")
DomCorPE <- PE %>% filter(outcome == "Dominance")
ValCorPE <- PE %>% filter(outcome == "Valence")

```

As seen in the graph below, prediction errors and image rating uncertainty (standard deviation) have correlations of `r round(cor(x = AroCorPE$PE, y = AroCorPE$rating_sd),3)` when the outcome is arousal, `r round(cor(x = DomCorPE$PE, y = DomCorPE$rating_sd),3)` when the outcome is dominance, and `r round(cor(x = ValCorPE$PE, y = ValCorPE$rating_sd),3)`, when the outcome is valence.

```{r prediction error plots, warning = FALSE, message = FALSE}

#Plot!
PE %>%
  ggplot(aes(x = rating_sd, y = PE, color = outcome)) +
  geom_point(show.legend = FALSE) +
  geom_smooth(method = "lm", show.legend = FALSE, color = "black", linetype = "dashed") +
  labs(y = "Absolute Value of the Prediction Error",
       x = "Standard Deviation",
       title = "Prediction Error and Uncertainty in Image Ratings") +
  facet_wrap(~outcome) +
  myGGTheme +
  scale_y_continuous(limits = c(-1,5)) +
  theme(plot.background = element_rect(fill = "aliceblue"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_line(color = "lightblue1"),
        panel.background = element_rect(fill = "aliceblue"),
        strip.background = element_rect(fill = "lightblue"),
        strip.text = element_text(face = "bold"))

```


## Choice Task

### Participants and Procedure

### Task Description

 The choice game was a 150-trial two-choice probabilistic reward task with emotionally-stimulating images from the IAPS database used as reinforcers. Each trial began with presentation of two icons on a computer screen, and participants were required to choose between them using a Logitech gaming controller. A total of 58 icons were randomly selected to be in one of six groups, with six unique icons per group (Table 1). Within each group, icons held fixed probabilities of displaying positive, neutral, or negative images as outcomes. The game was divided into three phases that the icon groups corresponded to, with 25 trails in phase one, 50 trails in phase 2, and 75 trials in phase 3 (Table 2).


```{r table 1, include = TRUE}
IAPSgroups1 <- read_csv('C:/Users/rachjone/Desktop/IAPS/Tables/IAPSgroups.csv')

flex <- flextable(head(IAPSgroups1)) %>%
set_header_labels(Group = "Group", Probabilities = "Probabilities") %>%
theme_vanilla() %>%
align(align = "center", part = "all") %>%
bg(bg = "cyan3", part = "header") %>%
bg(bg = "snow3", part = "body") %>%
color(color = "black", part = "all") %>%
bold(bold = TRUE, part = "header") %>%
fontsize(size = 14, part = "all") %>%
set_caption("Table 1: Icon Groupings") %>%
autofit()

plot(flex)
```

```{r table 2, include = TRUE}
IAPSphases1 <- read_csv('C:/Users/rachjone/Desktop/IAPS/Tables/IAPSphases.csv')

flex1 <- flextable(head(IAPSphases1)) %>%
  set_header_labels(Phase = "Phase", Trials = "Trials", Associated.Groups = "Associated Groups", Description = "Description") %>%
  theme_vanilla() %>%
  align(align = "center", part = "all") %>%
  bg(bg = "coral2", part = "header") %>%
  bg(bg = "snow3", part = "body") %>%
  color(color = "black", part = "all") %>%
  bold(bold = TRUE, part = "header") %>%
  fontsize(size = 14, part = "all") %>%
  set_caption("Table 2: Description of Phases in IAPS Choice Game") %>%
  autofit()

plot(flex1)
```

Phase one held positive-only icons, where fixed probabilities of showing positive images were pre-determined per icon (i.e. 25%, 50%, and 75% vs neutral). Phase two introduced negative-only icons with similar fixed probabilities of showing negative images per icon. In these phases, each round showed pairs of positive icons or pairs of negative icons exclusively. However, in phase 3, all icons were mixed so that any round may have consisted of two positive icons, two negative icons, or one positive and one negative icon. In this phase, outcome magnitudes changed giving rise to higher-complexity positive and negative images. That is, our selected positive and negative images were not equidistant from zero, but rather varied in complexity according to phase number. The probabilities associated with each icon remained the same throughout the game, meaning the expected value of each option is altered by phase 3. We employed this phase transition to understand temporal learning effects from images that vary in reward and complexity. The IAPS images used are presented in Appendix A according to valence, dominance, and arousal.


Participants were instructed to select their preferred icon and told that different types of images would be presented dependent upon their choice (Figure 1). They were not given instructions on the relationship between icons and images or the goal of the experiment. Before and after the game, participants were asked to select one of two icons they prefer with no image outcome. This was important to determine if certain characteristics of icons (i.e. color, laterality) influenced participants' choices throughout the game or if this changed because of image outcome.

<! -- ![Figure 1: Schematic of one trial in IAPS choice game.](C:/Users/rachjone/Pictures/Screenshots/Screenshot (686).png) -->

### Read in the Data

Below, we read in the data and preview the raw text file output from the choice task.

```{r read in data, cols.print = 2, message = FALSE, warning = FALSE}

choiceFiles <- dir_ls('~/Desktop/IAPS/RJT/choice/', glob = "*.txt.txt") #get the paths for the files that contain the string ".txt.txt". For new cohort.

rawChoiceData <- map(choiceFiles, ~readChoices(.x), .id = "subject")

#preview raw choice data
head(as.data.frame(rawChoiceData[[1]]), 20)

```

### Tidy the Data

Below, we tidy the data using the `iapsr` function `processChoiceData` and preview it. For information on how the processing was done, call `?iapsr::processChoiceData()` in the console. In summary, for each subject this function returns a dataframe 150 rows and 13 columns:

* phase: The phase the choices were made in.

* round: The round number these choices were made in, specific to the phase.

* runningRound: The round number these choices were made in (1-150), independent of phase. This is useful for plotting the percent of optimal choices over time.

* icon1: The icon presented in the first order position.

* rank1: The rank of icon1. The higher this is, the higher the expected utility of choosing icon1.

* icon2: The icon presented in the second order position.

* rank2: The rank of icon2. The higher this is, the higher the expected utility of choosing icon2.

* option: The option the subject chose. This should correspond to the icon order as presented in the task.

* choice: The icon the subject chose.

* group: The (payment weighting) group the chosen icon corresponds to.

* image: The image the subject was shown after making a choice.

* optimal: A binary variable tracking whether or not the choice was optimal. This is 1 if the rank of the icon chosen is higher than the rank of the one not chosen. It is 0 otherwise. The ranks are based on the expected utility for each icon. See getGroupInfo for that information.

* percentOptimal: The cumulative sum of optimal choices divided by the total round (1-150).

```{r tidy data - choice task, cols.print = 7}

finalChoiceData <- map_df(rawChoiceData, ~processChoiceData(.x), .id = "subject") %>%
  mutate(subject = str_extract(subject, "[:graph:]{9}(?=choice.txt)"))

finalChoiceData %>%
  head(10)
```

### Analyze Optimal Choices

Each subject is presented with a choice between two icons. In phase 1 there are three icons all leading to positively valenced images with varying probabilities. In phase 2 and 3 there are six icons; three lead to positively valenced images and three lead to negatively valenced images with differing probabilities. For instance, icon 1 might offer a reward equivalent to \$1.00 in valence with probability 0.25 and icon 2 might offer a reward equivalent to \$1.00 in valence with probability 0.5. Thus, the expected utility of icon 2 is greater than the expected utility of icon 1. This means that whenever an individual is presented with these two icons, the optimal choice is to choose icon 2.

Using the `getGroupInfo` function from the `iapsr` package, we get the reward-probability mappings for each icon per subject and then plot the cumulative sum of optimal choices.

#### Percent Optimal Plot

Below are two plots created using the `iapsr` function `plotPercentOptimal`. They show the percent of optimal choices taken by each of the `r params$numSubjects` subjects over the entire task. Phases are indicated. The second plot is faceted by subject.

```{r plot the data - combined, fig.align = "center", out.width = "90%"}

finalChoiceData %>% plotPercentOptimal(facet = FALSE)

```

```{r plot the data - faceted, fig.align = "center", out.width = "90%"}

finalChoiceData %>% plotPercentOptimal(facet = TRUE)

```
