---
title: "IAPS Regression Analysis"
author: "Jonathan Trattner, Rachel Jones, and Delaney Teceno"
date: "Last compiled on `r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    df_print: paged
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: true
      smooth_scroll: true
    theme: journal
    highlight: monochrome
    code_folding: hide
params:
  numSubjects: 5
---
```{r setup, include=FALSE}
#Load packages
library(tidyverse)
library(tidymodels)
library(fs)
library(tictoc)
library(vip)
library(caret)
library(iapsr) #our package, woohoo!

myGGTheme <- ggplot2::theme(plot.title = ggplot2::element_text(hjust=0.5, face = "bold.italic", size=16), #plot title aesthetics
                   plot.subtitle = ggplot2::element_text(hjust = 0.5, face = "bold", size = 12), #plot subtitle aesthetics
                   axis.title.x = ggplot2::element_text(size = 12, color= "black", face = "bold"), #x axis title aesthetics
                   axis.title.y = ggplot2::element_text(size = 12, color= "black", face = "bold"), #y axis title aesthetics
                   axis.text.x = ggplot2::element_text(angle = 0, hjust = 0.5, size = 12),
                   #legend aesthetics
                   legend.title = ggplot2::element_text(size= 14,
                                    color = "black",
                                    face = "bold"),
                   legend.title.align = 0.5,
                   legend.text = ggplot2::element_text(size = 10,
                                              color = "black",
                                              face = "bold"),
                   legend.text.align = 0)
```

## Introduction

In 2018, forty-eight subjects (original cohort) rated 120 images from the International Affective Picture System database which provides normative ratings on valence, arousal, and dominance. Our goal is to predict these normative ratings based on subjects' responses to the questions "How positive does this image make you feel?" and "How negative does this image make you feel?" This document provides reproducible code for each step in the data analysis pipeline, including reading the data, tidying the data, calculating models, and visualizing results.

## Read in Data

Below is the code to read in the data. The directories chunk is specific to the files' location on our computer and will need to be changed if run on another machine. The data is manipulated as it's read in -- please see the code comments for more detail.

```{r set directories}

dirIAPS <- '~/Desktop/IAPS/IAPS Data/' #set the directory for IAPS data
dirData <- '~/Desktop/IAPS/Subject Data/' #set the directory for subject data
dirImageOrder <- '~/Desktop/IAPS/Subject Image Order/' #set the directory for image order
dirRatings <- '~/Desktop/IAPS/RJT/ratings/' #for new cohort
fileRatings <- dir_ls(dirRatings, glob = "*.txt.txt") #get the paths for the files that contain the string ".txt.txt". For new cohort.
filesIAPS <- dir_ls(dirIAPS) #list the files in the IAPS directory
filesData <- dir_ls(dirData) #list the files in the subject data directory
filesImageOrder <- dir_ls(dirImageOrder) #list the files in the image order directory

```

```{r read data, message = FALSE, warning = FALSE}


#Read in the demographics data
demographics <- read_csv(file = "~/Desktop/IAPS/Demographics/KLRIF_2018-07-09.csv")

#Import the IAPS key data set
keyIAPS <- filesIAPS %>%
  map_df(~read_csv(.x, na = c("", ".", NA),
                   col_types = cols(IAPS = col_character())) %>%
           select(desc:dom1sd) %>%
           drop_na(),
         .id = "form") %>%
  mutate(form = case_when(str_detect(form, "1_IAPS") ~ 1,
                          str_detect(form, "2_IAPS") ~ 2,
                          str_detect(form, "3_IAPS") ~ 3,
                          str_detect(form, "4_IAPS") ~ 4),
         form = as.factor(form))

#Read in subject rating
subjectData <- filesData %>%
  map_df(~read_csv(.x) %>%
           rename(time = Timestamp,
                  subject = `Subject ID:`),
         .id = "form") %>%
  mutate(form = case_when(str_detect(form, "1_SFA") ~ 1,
                          str_detect(form, "2_SFA") ~ 2,
                          str_detect(form, "3_SFA") ~ 3,
                          str_detect(form, "4_SFA") ~ 4),
           form = as.factor(form)) %>%
  #pivot the data so all ratings are in a column "rating" instead of in their own rows.
  pivot_longer(cols = -c(form, time, subject), values_to = "rating") %>%
  select(-name) %>%
  #have a running order for each subject
  group_by(subject) %>%
  mutate(order = row_number()) %>%
  ungroup() %>%
  #make this subjectID string match that of imageOrder
  mutate(subject = str_replace(subject, "\\-", "\\_")) %>%
  select(-time) %>%
  #arrange by each subject and the form (so one subject goes through all four
  #forms before moving to next subject)
  arrange(subject, form)


  imageOrder <- filesImageOrder %>%
  map_df(~read_csv(.x),
         .id = "form") %>%
  mutate(form = case_when(str_detect(form, "1_pictureID") ~ 1,
                          str_detect(form, "2_pictureID") ~ 2,
                          str_detect(form, "3_pictureID") ~ 3,
                          str_detect(form, "4_pictureID") ~ 4),
         form = as.factor(form)) %>%
  #pivot the data so all image values (IDs) are in a column "value"
  pivot_longer(cols = -c(X1, form),
               names_to = "subject") %>%
  #remvoe subject prefix
  mutate(subject = str_remove(.$subject, "\\d\\_")) %>%
  #arrange by subject
  arrange(subject) %>%
  #remove NAs that appear because of how the data was read in where each form's
  #image ID were basically in a new column for the subject with the prefix
  #1_SFA_..."
  drop_na() %>%
  #have a running order for each subject, and remove the X1 column.
  group_by(subject) %>%
  mutate(order = row_number()) %>%
  ungroup() %>%
  select(-X1)
  
#Read in and manipulate the new cohort data.
newCohortRatings <- 
  #Read in all the ratings data into a list with each element corresponding to a
  #different file.
  map(fileRatings, ~readRatings(.x)) %>%
  #take each list element (subject file) and call our function
  #processRatingsData, creating an ID column called "subject". This column
  #simply contains the file path string name.
  map_df(~processRatingsData(.x), .id = "subject") %>%
  #extract from the subject column the IDs. For example "IAPS_B_01".
  mutate(subject = str_extract(subject, "[:graph:]{9}(?=rating.txt)")) 
  
#Read in and manipulate the new cohort demographics information. File path is
#dependent on user's computer.
newCohortDems <- read_csv('~/Desktop/IAPS/RJT/demographics/IAPSdems.csv') %>%
  mutate(gender = case_when(gender == "female" ~ 0, 
                            gender == "male" ~ 1))

```

## Tidy the Data

This section describes how we tidied the data in order to get it in a format to perform the elastic net regression. Please see code comments for more details.

```{r tidy data}

#tidy the demographics data
tidyDems <- demographics %>%
  #rename variables
  rename(subject = "Subject ID", 
         age = "Age (years)",
         gender = "What is your gender?") %>%
  #make subject characters line up with others and recode gender as binary for
  #regression purposes
  mutate(subject = str_replace(subject, "\\-", "\\_"),
         gender = recode(gender, "Male" = 1, "Female" = 0)) %>%
  #select only relevant variables
  select(subject, age, gender)


#join the imageOrder and subject data by subject, form, and order.
orderedSubjectData <- full_join(imageOrder, subjectData, by = c("form", "subject", "order")) %>%
  rename(picID = value)

#Label the orderedSubjectData with a column that has the type of question asked
#based on the picID's P or N. Then rename the picID column as IAPS, removing any
#"P" or "N" at the end, so the names are the same as in the keyIAPS dataframe.
labeledOrderedSubjectData <- orderedSubjectData %>%
  mutate(question = case_when(str_detect(picID, "P") ~ "positive",
                              str_detect(picID, "N") ~ "negative",
                              TRUE ~ "Oops"),
         picID = str_remove_all(picID, "P|N")) %>%
  rename(IAPS = picID)

#create positiveRatings as a dataframe that has IAPS data from the rounds where
#it was asked how positive the image made them feel. Then combine the rating and
#question data into one column that has the ratings for when the question was
#about positive feelings. I could've done this using the following code instead.
#Same thing.

#labeledOrderedSubjectData %>%
#  filter(question == "positive") %>%
#  select(-question) %>%
#  rename(positive = rating))

positiveRatings <- labeledOrderedSubjectData %>%
  filter(question == "positive") %>%
  pivot_wider(names_from = question, values_from = rating)

#create negativeRatings as a dataframe that has IAPS data from the rounds where
#it was asked how negative the image made them feel. Then combine the
#rating and question data into one column that has the ratings for when the
#question was about negative feelings.
negativeRatings <- labeledOrderedSubjectData %>%
  filter(question == "negative") %>%
  pivot_wider(names_from = question, values_from = rating)

#Combine the positive and negative ratings data by IAPS, subject, and form.
#Could've just used IAPS but just more specificity. Then remove miscellaneous
#order column and join that with the IAPS key. Then join that with the
#demographics info. Then remove NAs since there are some images participants saw
#that are not standardized in the IAPS key (and one participant who has no
#gender data and we exclude). The final data has 14 columns and 5640 rows and
#consists of 120 positive and negative ratings from 47 subjects.
finalData <- full_join(positiveRatings, negativeRatings, by = c("IAPS", "subject", "form")) %>%
  select(-c(order.x, order.y)) %>%
  full_join(keyIAPS, by = c("IAPS", "form")) %>%
  full_join(tidyDems, by = "subject") %>%
  drop_na() %>%
  select(subject, age, gender, 
         form, IAPS, desc, 
         positive, negative, everything())

# Create the final data to perform regressions by setting up the new cohort
# ratings data using the iapsr regSetup function, joining that with the new
# cohort demographics information by subject, and joining that by the IAPS
# ratings key.
newCohortFinalData <- newCohortRatings %>% 
  #remove a picture not in the IAPS key
  filter(picture != "7037") %>%
  regSetup() %>% 
  left_join(newCohortDems, by = "subject") %>%
  left_join(keyIAPS, by = "IAPS")

```

Below, we can see a preview of the data on which we will fit regression models. The first table shows the first 10 rows for subject SFA_1025 from the original cohort. The second table shows the first 10 rows for subject IAPS_B_01. The age column is in years, gender is binary (0 for females 1 for males for use in regression analysis), IAPS column is the picture ID. The desc column is a description of the IAPS images. The positive and negative columns show the subject's ratings when asked "How positive does this image make you feel?" and "How negative does this image make you feel?". The remaining columns describe the mean and standard deviation for the image's standardized valence, arousal, and dominance, per the IAPS dataset.

```{r preview, cols.print = 7, echo = FALSE}

#preview the regression data frame for subject SFA_1025
head(finalData, 10)

#preview the regression data frame for subject IAPS_B_01
head(newCohortFinalData, 10)

```

## Modeling the Data

### Methods and Code

Using both the `tidymodels` framework in R and the `caret` package in R, we tuned an elastic net regression with a wide array of alpha (0 to 1) and lambda (0 to 200) values using six fold and ten fold cross validation. Our response variable was the mean rating of valence, arousal, or dominance, per the IAPS dataset. Our predictors were subjects' positive and negative ratings (and the interaction between them), age, and gender. We looked at the root mean squared error (rmse), mean absolute error (mae), and the $R^2$ metrics to evaluate model fit on a dataset of `r params$numSubjects` subjects whose ratings were collected during the summer of 2020 (new cohort). These indicated that a regular linear regression performed best. We used both the `caret` package to perform 10 fold cross validation on the data, and the `lm` function in R fit on the entire dataset. The linear models performed equally well independent of cross validation. It should also be noted that we tested the linear model without the interaction between positive and negative ratings and it performed worse on all metrics. Included are code snippets for performing linear regressions using `lm` and `caret`. Elastic net regression code and model coefficients are available upon request.

```{r caret interaction, warning = FALSE, message = FALSE}

### Create a list of formulas for the outcome
outcomeFormulasCaret <- list("arousal" = aromn ~ positive + negative + (positive * negative) + age + gender,
                        "dominance" = dom1mn ~ positive + negative + (positive * negative) + age + gender,
                        "valence" = valmn ~ positive + negative + (positive * negative) + age + gender)

#Create strings vectors/lists that have the outcome strings and truths to supply in a map2 function for getting the metrics.
outcomeStrings <- list("arousal" = "arousal", "dominance" = "dominance", "valence" = "valence")
outcomeTruths <- c("aromn", "dom1mn", "valmn")

#create a train control object
careTrain <- trainControl(method = "cv", number = 10)

#set the seed and predict the data for each model using 10 fold cross validation
set.seed(18)
caretNewPredicted <- map(outcomeFormulasCaret, ~ train(.x, data = finalData, trControl = careTrain, method = "lm")) %>%
  map_df(~predict(.x, newCohortFinalData) %>% bind_cols(newCohortFinalData), .id = "outcome") %>%
  rename(predicted = `...1`)

#get the metrics for the caret model
caretNewPredMetrics <- map2_df(outcomeStrings, outcomeTruths, ~ caretNewPredicted %>% filter(outcome == .x) %>%
                                 metrics(truth = .y, estimate = predicted), .id = "outcome")

```

```{r caret - no interaction, warning = FALSE, message = FALSE}


### Create a list of formulas for the outcome
outcomeFormulasCaretNoInteraction <- list("arousal" = aromn ~ positive + negative + age + gender,
                        "dominance" = dom1mn ~ positive + negative + age + gender,
                        "valence" = valmn ~ positive + negative + age + gender)

#Create strings vectors/lists that have the outcome strings and truths to supply in a map2 function for getting the metrics.
outcomeStrings <- list("arousal" = "arousal", "dominance" = "dominance", "valence" = "valence")
outcomeTruths <- c("aromn", "dom1mn", "valmn")

#create a train control object
careTrain <- trainControl(method = "cv", number = 10)

#set the seed and predict the data for each model using 10 fold cross validation
set.seed(18)
caretNewPredictedNoInteraction <- map(outcomeFormulasCaretNoInteraction, ~ train(.x, data = finalData, trControl = careTrain, method = "lm")) %>%
  map_df(~predict(.x, newCohortFinalData) %>% bind_cols(newCohortFinalData), .id = "outcome") %>%
  rename(predicted = `...1`)

#get the metrics for the caret model
caretNewPredMetricsNoInteraction <- map2_df(outcomeStrings, outcomeTruths, ~ caretNewPredictedNoInteraction %>% filter(outcome == .x) %>%
                                 metrics(truth = .y, estimate = predicted), .id = "outcome")


```

```{r compare carets}

compareCarets <- bind_rows(caretNewPredMetricsNoInteraction %>% mutate(type = "no interaction"), 
                        caretNewPredMetrics %>% mutate(type = "interaction"))

```

```{r lm - no interaction, warning = FALSE, message = FALSE}

outcomeFormulasNoInteraction <- list("arousal" = lm(formula = aromn ~ positive + negative + age + gender, data = finalData),
                                     "dominance" = lm(formula = dom1mn ~ positive + negative + age + gender, data = finalData),
                                     "valence" = lm(formula = valmn ~ positive + negative + age + gender, data = finalData))


#Create strings vectors/lists that have the outcome strings and truths to supply in a map2 function for getting the metrics.
outcomeStrings <- list("arousal" = "arousal", "dominance" = "dominance", "valence" = "valence")
outcomeTruths <- c("aromn", "dom1mn", "valmn")


set.seed(18)
# Create a dataframe with the predictions for each based on the outcome
newCohortPredictedNoInteraction <- map_df(outcomeFormulasNoInteraction, ~ predict(.x, newCohortFinalData) %>% bind_cols(newCohortFinalData), .id = "outcome") %>%
  rename(predicted = `...1`)

#get metrics for each model's fit on the new data
newCohortPredictedMetricsNoInteraction <- map2_df(outcomeStrings, outcomeTruths, ~ newCohortPredictedNoInteraction %>% filter(outcome == .x) %>% metrics(truth = .y, estimate = predicted), .id = "outcome")


```

```{r lm - interaction, warning = FALSE, message = FALSE}

### Create a list of formulas for the outcome
outcomeFormulas <- list("arousal" = lm(formula = aromn ~ positive + negative + (positive * negative) + age + gender, data = finalData),
                        "dominance" = lm(formula = dom1mn ~ positive + negative + (positive * negative) + age + gender, data = finalData),
                        "valence" = lm(formula = valmn ~ positive + negative + (positive * negative) + age + gender, data = finalData))


set.seed(18)
# Create a dataframe with the predictions for each based on the outcome
newCohortPredicted <- map_df(outcomeFormulas, ~ predict(.x, newCohortFinalData) %>% bind_cols(newCohortFinalData), .id = "outcome") %>%
  rename(predicted = `...1`)

#get metrics for each model's fit on the new data
newCohortPredictedMetrics <- map2_df(outcomeStrings, outcomeTruths, ~ newCohortPredicted %>% filter(outcome == .x) %>%
                                       metrics(truth = .y, estimate = predicted), .id = "outcome")

```

```{r compare lms}

compareLMs <- bind_rows(newCohortPredictedMetricsNoInteraction %>% mutate(type = "no interaction"), 
                        newCohortPredictedMetrics %>% mutate(type = "interaction"))

```

```{r caret glmnet, include = FALSE}

# Just didn't feel like formatting this for markdown...

# # Caret GLMNET ------------------------------------------------------------
# 
# #caret glmnet interaction
# 
# ### Create a list of formulas for the outcome
# outcomeFormulasCaret <- list("arousal" = aromn ~ positive + negative + (positive * negative) + age + gender,
#                              "dominance" = dom1mn ~ positive + negative + (positive * negative) + age + gender,
#                              "valence" = valmn ~ positive + negative + (positive * negative) + age + gender)
# 
# #Create strings vectors/lists that have the outcome strings and truths to supply in a map2 function for getting the metrics.
# outcomeStrings <- list("arousal" = "arousal", "dominance" = "dominance", "valence" = "valence")
# outcomeTruths <- c("aromn", "dom1mn", "valmn")
# 
# #create a train control object
# careTrain <- trainControl(method = "cv", number = 10)
# caretGrid <- expand.grid(alpha = seq(0, 1, by = 0.1),
#   lambda = c(seq(0, 1, length = 20), seq(2,10, by = 1), seq(15,200, by = 5)))
# 
# #set the seed and predict the data for each model using 10 fold cross validation
# set.seed(18)
# caretGlmnetNewPredicted <- map(outcomeFormulasCaret, ~ train(.x, data = finalData, method = "glmnet", trControl = careTrain, tuneGrid = caretGrid)) %>%
#   map_df(~predict(.x, newCohortFinalData) %>% bind_cols(newCohortFinalData), .id = "outcome") %>%
#   rename(predicted = `...1`)
# 
# #get the metrics for the caret model
# caretGlmnetNewPredMetrics <- map2_df(outcomeStrings, outcomeTruths, ~ caretGlmnetNewPredicted %>% filter(outcome == .x) %>%
#                                  metrics(truth = .y, estimate = predicted), .id = "outcome")
# 
# 
# 
# # Caret glmnet No interaction
# 
# ### Create a list of formulas for the outcome
# outcomeFormulasCaretNoInteraction <- list("arousal" = aromn ~ positive + negative + age + gender,
#                              "dominance" = dom1mn ~ positive + negative + age + gender,
#                              "valence" = valmn ~ positive + negative + age + gender)
# 
# #Create strings vectors/lists that have the outcome strings and truths to supply in a map2 function for getting the metrics.
# outcomeStrings <- list("arousal" = "arousal", "dominance" = "dominance", "valence" = "valence")
# outcomeTruths <- c("aromn", "dom1mn", "valmn")
# 
# #create a train control object
# careTrain <- trainControl(method = "cv", number = 10)
# caretGrid <- expand.grid(alpha = seq(0, 1, by = 0.1),
#                          lambda = c(seq(0, 1, length = 20), seq(2,10, by = 1), seq(15,200, by = 5)))
# 
# #set the seed and predict the data for each model using 10 fold cross validation
# set.seed(18)
# caretGlmnetNewPredNoInteraction <- map(outcomeFormulasCaretNoInteraction, ~ train(.x, data = finalData, method = "glmnet", trControl = careTrain, tuneGrid = caretGrid)) %>%
#   map_df(~predict(.x, newCohortFinalData) %>% bind_cols(newCohortFinalData), .id = "outcome") %>%
#   rename(predicted = `...1`)
# 
# #get the metrics for the caret model
# caretGlmnetNewPredMetricsNoInteraction <- map2_df(outcomeStrings, outcomeTruths, ~ caretGlmnetNewPredNoInteraction %>% filter(outcome == .x) %>%
#                                  metrics(truth = .y, estimate = predicted), .id = "outcome")
# 
# 
# 
# 
# ### compare caret glmnets
# compareCaretsGlmnet <- bind_rows(caretGlmnetNewPredMetricsNoInteraction %>% mutate(type = "no interaction"), 
#                            caretGlmnetNewPredMetrics %>% mutate(type = "interaction"))
# 

```

### Performance Metrics

To reiterate, our best model is a simple linear regression that predicts the mean arousal, dominance, and valence level using subjects' positive and negative ratings (and the interaction between them), age, and gender. The model statistics can be seen in the tables below.

```{r lm - interaction metrics}

print("Linear Regression Coefficients for Predicting Arousal:")
outcomeFormulas$arousal %>% tidy()

print("Linear Regression Coefficients for Predicting Dominance:")
outcomeFormulas$dominance %>% tidy()

print("Linear Regression Coefficients for Predicting Valence:")
outcomeFormulas$valence %>% tidy()

print("Linear Regression Prediction Metrics:")
newCohortPredictedMetrics

```

### How do the predictions correlate?

```{r actual vs predicted data frame, warning = FALSE, message = FALSE}

#The actual column is "rating_mean" and the predicted column is "predicted"
actual_predicted <- newCohortPredicted %>%
  pivot_longer(cols = contains("mn"), names_to = "mean_type", values_to = "rating_mean") %>%
  pivot_longer(cols = contains("sd"), names_to = "sd_type", values_to = "rating_sd") %>%
  filter((outcome == "arousal" & mean_type == "aromn" & sd_type == "arosd") |
           (outcome == "dominance" & mean_type == "dom1mn" & sd_type == "dom1sd") |
           (outcome == "valence" & mean_type == "valmn" & sd_type == "valsd")) %>% 
  mutate(outcome = str_to_sentence(outcome))
  
```
  
```{r correlations - pred/actual}

AroCor <- actual_predicted %>% filter(outcome == "Arousal") 
DomCor <- actual_predicted %>% filter(outcome == "Dominance")
ValCor <- actual_predicted %>% filter(outcome == "Valence")

```

In order to check how accurate our model is, we can plot the actual vs. predicted outcomes and observe the trends. We do this only for the best fitting model as outlined in the above section. The first plot is an average over all `r params$numSubjects` subjects in the new cohort. The predicted and actual ratings have an overall correlation of `r round(cor(x = AroCor$rating_mean, y = AroCor$predicted),3)` when the outcome is arousal, `r round(cor(x = DomCor$rating_mean, y = DomCor$predicted),3)` when the outcome is dominance, and `r round(cor(x = ValCor$rating_mean, y = ValCor$predicted),3)`, when the outcome is valence. The second plot shows the individual variations in these relationships.

```{r actual vs predicted plot, warning = FALSE, message = FALSE}
  
#On the whole
actual_predicted %>%
  ggplot(aes(x = rating_mean, y = predicted, color = outcome)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~outcome) +
  scale_y_continuous(limits = c(1,9), breaks = c(seq(1,9))) +
  scale_x_continuous(limits = c(1,9), breaks = c(seq(1,9))) +
  geom_smooth(method = "lm", show.legend = FALSE, color = "black", linetype = "dashed") +
  labs(title = "Predicted IAPS Rating vs Actual Rating",
       y = "Predicted Rating",
       x = "Actual Rating") +
  myGGTheme +
  theme(plot.background = element_rect(fill = "aliceblue"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_line(color = "lightblue1"),
        panel.background = element_rect(fill = "aliceblue"),
        strip.background = element_rect(fill = "lightblue"),
        strip.text = element_text(face = "bold"))

```



```{r actual vs predicted by subject plot, warning = FALSE, message = FALSE}
  
#On the whole
actual_predicted %>%
  ggplot(aes(x = rating_mean, y = predicted, color = outcome)) +
  geom_point(show.legend = FALSE) +
  facet_grid(outcome ~ subject, switch = "y") +
  scale_y_continuous(limits = c(1,9), breaks = c(seq(1,9))) +
  scale_x_continuous(limits = c(1,9), breaks = c(seq(1,9))) +
  geom_smooth(method = "lm", show.legend = FALSE, color = "black", linetype = "dashed") +
  labs(title = "Predicted IAPS Rating vs Actual Rating",
       subtitle = "Faceted by Every Subject in the New Cohort",
       y = "Predicted Rating",
       x = "Actual Rating") +
  myGGTheme +
  theme(plot.background = element_rect(fill = "aliceblue"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_line(color = "lightblue1"),
        panel.background = element_rect(fill = "aliceblue"),
        strip.background = element_rect(fill = "lightblue"),
        strip.text = element_text(face = "bold"))

```



### Do the prediction errors vary with rating uncertainty?

Another question to ask is whether or not the prediction errors (the difference between actual and predicted rating) varies with the uncertainty, as measured by the standard deviation, of each rating from the IAPS dataset.

```{r prediction errors}

#create a prediction error data frame that has the difference between the
#predicted and actual outcomes. 
PE <- actual_predicted %>% 
  mutate(PE = predicted - rating_mean,
         PE = abs(PE)) %>%
  select(PE, rating_sd, outcome)

```

```{r correlations PE/SD}

AroCorPE <- PE %>% filter(outcome == "Arousal") 
DomCorPE <- PE %>% filter(outcome == "Dominance")
ValCorPE <- PE %>% filter(outcome == "Valence")

```

As seen in the graph below, prediction errors and image rating uncertainty (standard deviation) have correlations of `r round(cor(x = AroCorPE$PE, y = AroCorPE$rating_sd),3)` when the outcome is arousal, `r round(cor(x = DomCorPE$PE, y = DomCorPE$rating_sd),3)` when the outcome is dominance, and `r round(cor(x = ValCorPE$PE, y = ValCorPE$rating_sd),3)`, when the outcome is valence.

```{r prediction error plots, warning = FALSE, message = FALSE}

#Plot!
PE %>%
  ggplot(aes(x = rating_sd, y = PE, color = outcome)) +
  geom_point(show.legend = FALSE) +
  geom_smooth(method = "lm", show.legend = FALSE, color = "black", linetype = "dashed") +
  labs(y = "Absolute Value of the Prediction Error", 
       x = "Standard Deviation",
       title = "Prediction Error and Uncertainty in Image Ratings") + 
  facet_wrap(~outcome) +
  myGGTheme +
  scale_y_continuous(limits = c(-1,5)) +
  theme(plot.background = element_rect(fill = "aliceblue"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_line(color = "lightblue1"),
        panel.background = element_rect(fill = "aliceblue"),
        strip.background = element_rect(fill = "lightblue"),
        strip.text = element_text(face = "bold"))

```
